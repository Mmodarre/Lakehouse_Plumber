# Generated by LakehousePlumber
# Pipeline: acmi_edw_bronze
# FlowGroup: orders_bronze

from pyspark import pipelines as dp
from pyspark.sql import functions as F

# Pipeline Configuration
PIPELINE_ID = "acmi_edw_bronze"
FLOWGROUP_ID = "orders_bronze"


# ============================================================================
# SOURCE VIEWS
# ============================================================================


@dp.temporary_view()
def v_orders_africa_raw():
    """Load orders Africa table from raw schema"""
    df = spark.readStream.table("acme_edw_prod.edw_raw.orders_africa_raw")

    # Add operational metadata columns
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


@dp.temporary_view()
def v_orders_america_raw():
    """Load orders America table from raw schema"""
    df = spark.readStream.table("acme_edw_prod.edw_raw.orders_america_raw")

    # Add operational metadata columns
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


@dp.temporary_view()
def v_orders_asia_raw():
    """Load orders Asia table from raw schema"""
    df = spark.readStream.table("acme_edw_prod.edw_raw.orders_asia_raw")

    # Add operational metadata columns
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


@dp.temporary_view()
def v_orders_europe_raw():
    """Load orders Europe table from raw schema"""
    df = spark.readStream.table("acme_edw_prod.edw_raw.orders_europe_raw")

    # Add operational metadata columns
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


@dp.temporary_view()
def v_orders_middle_east_raw():
    """Load orders Middle East table from raw schema"""
    df = spark.readStream.table("acme_edw_prod.edw_raw.orders_middle_east_raw")

    # Add operational metadata columns
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


@dp.temporary_view()
def v_orders_migration():
    """Load orders table from migration schema"""
    df = spark.read.table("acme_edw_prod.edw_old.orders")

    # Add operational metadata columns
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


# ============================================================================
# TRANSFORMATION VIEWS
# ============================================================================


@dp.temporary_view(comment="SQL transform: orders_africa_bronze_cleanse")
def v_orders_africa_bronze_cleaned():
    """SQL transform: orders_africa_bronze_cleanse"""
    df = spark.sql(
        """SELECT
  o_orderkey as order_id,
  o_custkey as customer_id,
  o_orderstatus as order_status,
  o_totalprice as total_price,
  o_orderdate as order_date,
  o_orderpriority as order_priority,
  o_clerk as clerk,
  o_shippriority as ship_priority,
  o_comment as comment,
  cast(last_modified_dt as TIMESTAMP) as last_modified_dt,
  * EXCEPT(o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, last_modified_dt,_rescued_data)
FROM stream(v_orders_africa_raw)"""
    )

    return df


@dp.temporary_view(comment="SQL transform: orders_america_bronze_cleanse")
def v_orders_america_bronze_cleaned():
    """SQL transform: orders_america_bronze_cleanse"""
    df = spark.sql(
        """SELECT
  o_orderkey as order_id,
  o_custkey as customer_id,
  o_orderstatus as order_status,
  o_totalprice as total_price,
  o_orderdate as order_date,
  o_orderpriority as order_priority,
  o_clerk as clerk,
  o_shippriority as ship_priority,
  o_comment as comment,
  cast(last_modified_dt as TIMESTAMP) as last_modified_dt,
  * EXCEPT(o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, last_modified_dt,_rescued_data)
FROM stream(v_orders_america_raw)"""
    )

    return df


@dp.temporary_view(comment="SQL transform: orders_asia_bronze_cleanse")
def v_orders_asia_bronze_cleaned():
    """SQL transform: orders_asia_bronze_cleanse"""
    df = spark.sql(
        """SELECT
  o_orderkey as order_id,
  o_custkey as customer_id,
  o_orderstatus as order_status,
  o_totalprice as total_price,
  o_orderdate as order_date,
  o_orderpriority as order_priority,
  o_clerk as clerk,
  o_shippriority as ship_priority,
  o_comment as comment,
  cast(last_modified_dt as TIMESTAMP) as last_modified_dt,
  * EXCEPT(o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, last_modified_dt,_rescued_data)
FROM stream(v_orders_asia_raw)"""
    )

    return df


@dp.temporary_view(comment="SQL transform: orders_europe_bronze_cleanse")
def v_orders_europe_bronze_cleaned():
    """SQL transform: orders_europe_bronze_cleanse"""
    df = spark.sql(
        """SELECT
  o_orderkey as order_id,
  o_custkey as customer_id,
  o_orderstatus as order_status,
  o_totalprice as total_price,
  o_orderdate as order_date,
  o_orderpriority as order_priority,
  o_clerk as clerk,
  o_shippriority as ship_priority,
  o_comment as comment,
  cast(last_modified_dt as TIMESTAMP) as last_modified_dt,
  * EXCEPT(o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, last_modified_dt,_rescued_data)
FROM stream(v_orders_europe_raw)"""
    )

    return df


@dp.temporary_view(comment="SQL transform: orders_middle_east_bronze_cleanse")
def v_orders_middle_east_bronze_cleaned():
    """SQL transform: orders_middle_east_bronze_cleanse"""
    df = spark.sql(
        """SELECT
  o_orderkey as order_id,
  o_custkey as customer_id,
  o_orderstatus as order_status,
  o_totalprice as total_price,
  o_orderdate as order_date,
  o_orderpriority as order_priority,
  o_clerk as clerk,
  o_shippriority as ship_priority,
  o_comment as comment,
  cast(last_modified_dt as TIMESTAMP) as last_modified_dt,
  * EXCEPT(o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, last_modified_dt,_rescued_data)
FROM stream(v_orders_middle_east_raw)"""
    )

    return df


@dp.temporary_view(comment="SQL transform: orders_migration_cleanse")
def v_orders_migration_cleaned():
    """SQL transform: orders_migration_cleanse"""
    df = spark.sql(
        """SELECT
  o_orderkey as order_id,
  o_custkey as customer_id,
  o_orderstatus as order_status,
  o_totalprice as total_price,
  o_orderdate as order_date,
  o_orderpriority as order_priority,
  o_clerk as clerk,
  o_shippriority as ship_priority,
  o_comment as comment,
  cast(last_modified_dt as TIMESTAMP) as last_modified_dt,
  'MIGRATION' as _source_file_path,
  * EXCEPT(o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, last_modified_dt)
FROM v_orders_migration"""
    )

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================

# Create the streaming table
dp.create_streaming_table(
    name="acme_edw_prod.edw_bronze.orders",
    comment="Streaming table: orders",
    table_properties={"delta.enableRowTracking": "true"},
)


# Define append flow(s)
@dp.append_flow(
    target="acme_edw_prod.edw_bronze.orders",
    name="f_orders_africa_bronze",
    comment="Append flow to acme_edw_prod.edw_bronze.orders",
)
def f_orders_africa_bronze():
    """Append flow to acme_edw_prod.edw_bronze.orders"""
    # Streaming flow
    df = spark.readStream.table("v_orders_africa_bronze_cleaned")

    return df


@dp.append_flow(
    target="acme_edw_prod.edw_bronze.orders",
    name="f_orders_america_bronze",
    comment="Append flow to acme_edw_prod.edw_bronze.orders",
)
def f_orders_america_bronze():
    """Append flow to acme_edw_prod.edw_bronze.orders"""
    # Streaming flow
    df = spark.readStream.table("v_orders_america_bronze_cleaned")

    return df


@dp.append_flow(
    target="acme_edw_prod.edw_bronze.orders",
    name="f_orders_asia_bronze",
    comment="Append flow to acme_edw_prod.edw_bronze.orders",
)
def f_orders_asia_bronze():
    """Append flow to acme_edw_prod.edw_bronze.orders"""
    # Streaming flow
    df = spark.readStream.table("v_orders_asia_bronze_cleaned")

    return df


@dp.append_flow(
    target="acme_edw_prod.edw_bronze.orders",
    name="f_orders_europe_bronze",
    comment="Append flow to acme_edw_prod.edw_bronze.orders",
)
def f_orders_europe_bronze():
    """Append flow to acme_edw_prod.edw_bronze.orders"""
    # Streaming flow
    df = spark.readStream.table("v_orders_europe_bronze_cleaned")

    return df


@dp.append_flow(
    target="acme_edw_prod.edw_bronze.orders",
    name="f_orders_middle_east_bronze",
    comment="Append flow to acme_edw_prod.edw_bronze.orders",
)
def f_orders_middle_east_bronze():
    """Append flow to acme_edw_prod.edw_bronze.orders"""
    # Streaming flow
    df = spark.readStream.table("v_orders_middle_east_bronze_cleaned")

    return df


@dp.append_flow(
    target="acme_edw_prod.edw_bronze.orders",
    name="f_orders_migration",
    once=True,
    comment="Append flow to acme_edw_prod.edw_bronze.orders",
)
def f_orders_migration():
    """Append flow to acme_edw_prod.edw_bronze.orders"""
    # One-time flow (backfill)
    df = spark.read.table("v_orders_migration_cleaned")

    return df
