# Generated by LakehousePlumber
# Pipeline: acmi_edw_tests
# FlowGroup: gold_layer_tests

from pyspark.sql.functions import *
import dlt

# Pipeline Configuration
PIPELINE_ID = "acmi_edw_tests"
FLOWGROUP_ID = "gold_layer_tests"


# ============================================================================
# DATA QUALITY TESTS
# ============================================================================


@dlt.expect_all_or_fail(
    {
        "valid_avg_order_value": "abs(avg_order_value - (total_revenue / total_orders)) < 0.01",
        "positive_metrics": "total_orders > 0 AND total_revenue > 0",
        "date_consistency": "first_order_date <= last_order_date",
    }
)
@dlt.table(
    name="tmp_test_test_customer_ltv_calculations",
    comment="Validate customer lifetime value calculations are correct",
    temporary=True,
)
def tmp_test_test_customer_ltv_calculations():
    """Validate customer lifetime value calculations are correct"""
    return spark.sql(
        """
        SELECT
          customer_id,
          total_orders,
          total_revenue,
          avg_order_value,
          first_order_date,
          last_order_date
        FROM acme_edw_tst.edw_gold.customer_lifetime_value
        WHERE total_orders > 0
    """
    )


@dlt.expect_all({"all_customers_segmented": "total_customers = segmented_customers"})
@dlt.table(
    name="tmp_test_test_customer_segmentation_coverage",
    comment="Ensure all customers are included in segmentation",
    temporary=True,
)
def tmp_test_test_customer_segmentation_coverage():
    """Ensure all customers are included in segmentation"""
    return spark.sql(
        """
        WITH all_customers AS (
          SELECT DISTINCT customer_sk FROM acme_edw_tst.edw_silver.customer_dim
        ),
        segmented_customers AS (
          SELECT DISTINCT customer_sk FROM acme_edw_tst.edw_gold.customer_segmentation_mv
        )
        SELECT
          (SELECT COUNT(*) FROM all_customers) as total_customers,
          (SELECT COUNT(*) FROM segmented_customers) as segmented_customers
    """
    )


@dlt.expect_all_or_fail(
    {
        "metrics_consistency": "abs(avg_order_value - (total_revenue / total_orders)) < 0.01",
        "positive_counts": "total_orders >= 0 AND total_customers >= 0",
    }
)
@dlt.table(
    name="tmp_test_test_executive_dashboard_consistency",
    comment="Validate executive dashboard calculations",
    temporary=True,
)
def tmp_test_test_executive_dashboard_consistency():
    """Validate executive dashboard calculations"""
    return spark.sql(
        """
        SELECT
          reporting_date,
          total_revenue,
          total_orders,
          total_customers,
          avg_order_value
        FROM acme_edw_tst.edw_gold.executive_dashboard_mv
        WHERE reporting_date >= dateadd(day, -30, current_date())
    """
    )


@dlt.expect_all_or_fail(
    {
        "valid_pricing": "avg_selling_price > 0 AND avg_selling_price = (total_revenue / total_quantity_sold)"
    }
)
@dlt.expect_all(
    {"reasonable_margins": "profit_margin >= -1.0 AND profit_margin <= 1.0"}
)
@dlt.table(
    name="tmp_test_test_product_performance",
    comment="Validate product performance calculations",
    temporary=True,
)
def tmp_test_test_product_performance():
    """Validate product performance calculations"""
    return spark.sql(
        """
        SELECT
          product_id,
          total_quantity_sold,
          total_revenue,
          avg_selling_price,
          profit_margin
        FROM acme_edw_tst.edw_gold.product_performance_mv
    """
    )


@dlt.expect_all_or_fail({"referential_integrity": "ref_c_custkey IS NOT NULL"})
@dlt.table(
    name="tmp_test_test_mv_referential_integrity",
    comment="All customers in gold layer must exist in silver dimensions",
    temporary=True,
)
def tmp_test_test_mv_referential_integrity():
    """All customers in gold layer must exist in silver dimensions"""
    return spark.sql(
        """
        SELECT
                      s.*,
                      r.c_custkey as ref_c_custkey
                    FROM acme_edw_tst.edw_gold.customer_lifetime_value s
                    LEFT JOIN acme_edw_tst.edw_silver.customer_dim r ON s.customer_id = r.c_custkey
    """
    )


@dlt.expect_all_or_fail({"revenue_matches": "difference < 0.01"})
@dlt.table(
    name="tmp_test_test_revenue_reconciliation",
    comment="Ensure revenue totals match between silver and gold layers",
    temporary=True,
)
def tmp_test_test_revenue_reconciliation():
    """Ensure revenue totals match between silver and gold layers"""
    return spark.sql(
        """
        WITH silver_revenue AS (
          SELECT
            DATE(o_orderdate) as order_date,
            SUM(l_extendedprice * (1 - l_discount)) as silver_total
          FROM acme_edw_tst.edw_silver.lineitem_fct l
          JOIN acme_edw_tst.edw_silver.orders_fct o ON l.l_orderkey = o.o_orderkey
          GROUP BY DATE(o_orderdate)
        ),
        gold_revenue AS (
          SELECT
            reporting_date,
            total_revenue as gold_total
          FROM acme_edw_tst.edw_gold.executive_dashboard_mv
        )
        SELECT
          g.reporting_date,
          g.gold_total,
          s.silver_total,
          abs(g.gold_total - s.silver_total) as difference
        FROM gold_revenue g
        LEFT JOIN silver_revenue s ON g.reporting_date = s.order_date
        WHERE s.silver_total IS NOT NULL
    """
    )


@dlt.expect_all_or_fail(
    {
        "value_in_range": "reporting_date >= '2020-01-01' AND reporting_date <= 'current_date()'"
    }
)
@dlt.table(
    name="tmp_test_test_aggregation_date_ranges",
    comment="Ensure dashboard dates are within expected range",
    temporary=True,
)
def tmp_test_test_aggregation_date_ranges():
    """Ensure dashboard dates are within expected range"""
    return spark.sql(
        """
        SELECT reporting_date
                    FROM acme_edw_tst.edw_gold.executive_dashboard_mv
    """
    )
