# Generated by LakehousePlumber
# Pipeline: acmi_edw_modelled
# FlowGroup: partsupp_modelled_fct

from pyspark import pipelines as dp
from pyspark.sql import DataFrame

# Pipeline Configuration
PIPELINE_ID = "acmi_edw_modelled"
FLOWGROUP_ID = "partsupp_modelled_fct"


# ============================================================================
# SOURCE VIEWS
# ============================================================================


@dp.temporary_view()
def v_partsupp_modelled_fct():
    """Load partsupp table with surrogate keys"""
    df = spark.sql(
        """select
p.part_key,
s.supplier_key,
ps.* except(end_week_key,start_week_key)
from
  (
    select
      *,
      cast(replace(`__START_AT`, '-W', '') as int) as `start_week_key`,
      coalesce(cast(replace(`__END_AT`, '-W', '') as int),99999999) as `end_week_key`
    from
      acme_edw_tst.edw_silver.partsupp_dim
  ) ps
    join (
      select
        *,
        cast(replace(`__START_AT`, '-W', '') as int) as `start_week_key`,
      coalesce(cast(replace(`__END_AT`, '-W', '') as int),99999999) as `end_week_key`
      from
        acme_edw_tst.edw_silver.supplier_dim
    ) s
      on ps.supplier_id = s.supplier_id
      and ps.start_week_key between s.start_week_key and s.end_week_key
    join (
      select
        *,
        CAST(
          EXTRACT(YEAROFWEEK FROM `__START_AT`) * 100 + EXTRACT(WEEK FROM `__START_AT`) AS INTEGER
        ) AS start_week_key,
        coalesce(CAST(
          EXTRACT(YEAROFWEEK FROM `__END_AT`) * 100 + EXTRACT(WEEK FROM `__END_AT`) AS INTEGER
        ),99999999) AS end_week_key
      from
        acme_edw_tst.edw_silver.part_dim
    ) p
    on ps.part_id = p.part_id
"""
    )

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================


@dp.materialized_view(
    name="acme_edw_tst.edw_gold.partsupp_modelled_fct",
    comment="Materialized view: partsupp_modelled_fct",
    table_properties={},
)
def partsupp_modelled_fct():
    """Write to acme_edw_tst.edw_gold.partsupp_modelled_fct from multiple sources"""
    # Materialized views use batch processing
    df = spark.read.table("v_partsupp_modelled_fct")

    return df
