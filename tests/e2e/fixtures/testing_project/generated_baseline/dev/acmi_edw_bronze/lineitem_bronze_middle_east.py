# Generated by LakehousePlumber
# Pipeline: acmi_edw_bronze
# FlowGroup: lineitem_bronze_middle_east

from pyspark import pipelines as dp
from pyspark.sql import functions as F
from pyspark.sql.types import StructType

# Pipeline Configuration
PIPELINE_ID = "acmi_edw_bronze"
FLOWGROUP_ID = "lineitem_bronze_middle_east"


# ============================================================================
# SOURCE VIEWS
# ============================================================================


@dp.temporary_view()
def v_lineitem_middle_east_raw():
    """Load lineitem table from raw schema"""
    df = spark.readStream.table("acme_edw_dev.edw_raw.lineitem_middle_east_raw")

    # Add operational metadata columns
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


# ============================================================================
# TRANSFORMATION VIEWS
# ============================================================================


@dp.temporary_view()
def v_lineitem_middle_east_bronze_cleaned():
    """Schema application: lineitem_bronze_cleanse"""
    df = spark.readStream.table("v_lineitem_middle_east_raw")

    # Apply column renaming
    df = df.withColumnRenamed("l_orderkey", "order_id")
    df = df.withColumnRenamed("l_partkey", "part_id")
    df = df.withColumnRenamed("l_suppkey", "supplier_id")
    df = df.withColumnRenamed("l_linenumber", "line_number")
    df = df.withColumnRenamed("l_quantity", "quantity")
    df = df.withColumnRenamed("l_extendedprice", "extended_price")
    df = df.withColumnRenamed("l_discount", "discount")
    df = df.withColumnRenamed("l_tax", "tax")
    df = df.withColumnRenamed("l_returnflag", "return_flag")
    df = df.withColumnRenamed("l_linestatus", "line_status")
    df = df.withColumnRenamed("l_shipdate", "ship_date")
    df = df.withColumnRenamed("l_commitdate", "commit_date")
    df = df.withColumnRenamed("l_receiptdate", "receipt_date")
    df = df.withColumnRenamed("l_shipinstruct", "ship_instruct")
    df = df.withColumnRenamed("l_shipmode", "ship_mode")
    df = df.withColumnRenamed("l_comment", "comment")

    # Apply type casting
    df = df.withColumn("order_id", F.col("order_id").cast("BIGINT"))
    df = df.withColumn("part_id", F.col("part_id").cast("BIGINT"))
    df = df.withColumn("supplier_id", F.col("supplier_id").cast("BIGINT"))
    df = df.withColumn("line_number", F.col("line_number").cast("INT"))
    df = df.withColumn("quantity", F.col("quantity").cast("DECIMAL(18,2)"))
    df = df.withColumn("extended_price", F.col("extended_price").cast("DECIMAL(18,2)"))
    df = df.withColumn("discount", F.col("discount").cast("DECIMAL(18,2)"))
    df = df.withColumn("tax", F.col("tax").cast("DECIMAL(18,2)"))
    df = df.withColumn("return_flag", F.col("return_flag").cast("STRING"))
    df = df.withColumn("line_status", F.col("line_status").cast("STRING"))
    df = df.withColumn("ship_date", F.col("ship_date").cast("DATE"))
    df = df.withColumn("commit_date", F.col("commit_date").cast("DATE"))
    df = df.withColumn("receipt_date", F.col("receipt_date").cast("DATE"))
    df = df.withColumn("ship_instruct", F.col("ship_instruct").cast("STRING"))
    df = df.withColumn("ship_mode", F.col("ship_mode").cast("STRING"))
    df = df.withColumn("comment", F.col("comment").cast("STRING"))
    df = df.withColumn("last_modified_dt", F.col("last_modified_dt").cast("TIMESTAMP"))

    return df


@dp.temporary_view()
# These expectations will fail the pipeline if violated
@dp.expect_all_or_fail(
    {
        "valid_order_id": "order_id IS NOT NULL AND order_id > 0",
        "valid_part_id": "part_id IS NOT NULL AND part_id > 0",
        "valid_supplier_id": "supplier_id IS NOT NULL AND supplier_id > 0",
        "valid_linenumber": "line_number IS NOT NULL AND line_number > 0",
        "valid_quantity": "quantity IS NOT NULL AND quantity > 0",
        "valid_extendedprice": "extended_price IS NOT NULL AND extended_price > 0",
    }
)
# These expectations will log warnings but not drop rows
@dp.expect_all(
    {
        "valid_discount": "discount IS NOT NULL AND discount > 0",
        "valid_tax": "tax IS NOT NULL AND tax > 0",
        "valid_return_flag": "return_flag IS NULL OR return_flag IN ('R', 'N')",
        "valid_line_status": "line_status IS NULL OR line_status IN ('F', 'O')",
        "valid_ship_date": "(ship_date IS NOT NULL AND ship_date >= '1900-01-01') or ship_date is null",
        "valid_commit_date": "commit_date IS NOT NULL AND commit_date >= '1900-01-01'",
        "valid_receipt_date": "(receipt_date IS NOT NULL AND receipt_date >= '1900-01-01') or receipt_date is null",
        "valid_ship_mode": "ship_mode IS NOT NULL AND ship_mode IN ('RAIL','REG AIR','TRUCK','MAIL','SHIP','FOB')",
    }
)
def v_lineitem_middle_east_bronze_DQE():
    """Apply data quality checks to lineitem"""
    df = spark.readStream.table("v_lineitem_middle_east_bronze_cleaned")

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================


# Define append flow(s)
@dp.append_flow(
    target="acme_edw_dev.edw_bronze.lineitem",
    name="f_lineitem_middle_east_bronze",
    comment="Append flow to acme_edw_dev.edw_bronze.lineitem",
)
def f_lineitem_middle_east_bronze():
    """Append flow to acme_edw_dev.edw_bronze.lineitem"""
    # Streaming flow
    df = spark.readStream.table("v_lineitem_middle_east_bronze_DQE")

    return df
