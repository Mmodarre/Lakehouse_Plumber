# Generated by LakehousePlumber
# Pipeline: kafka_sample_pipeline
# FlowGroup: kafka_orders_ingestion

from pyspark import pipelines as dp

# Pipeline Configuration
PIPELINE_ID = "kafka_sample_pipeline"
FLOWGROUP_ID = "kafka_orders_ingestion"


# ============================================================================
# SOURCE VIEWS
# ============================================================================


@dp.temporary_view()
def v_kafka_orders_raw():
    """Load orders from Kafka topics matching pattern"""
    df = (
        spark.readStream.format("kafka")
        .option(
            "kafka.bootstrap.servers", "pkc-921jm.us-east-2.aws.confluent.cloud:9092"
        )
        .option("subscribe", "sample_data_orders")
        .option("startingOffsets", "earliest")
        .option("failOnDataLoss", False)
        .option("kafka.security.protocol", "SASL_SSL")
        .option("kafka.sasl.mechanism", "PLAIN")
        .option(
            "kafka.sasl.jaas.config",
            f"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{dbutils.secrets.get(scope='kafka_secrets', key='kafka_api_key')}\" password=\"{dbutils.secrets.get(scope='kafka_secrets', key='kafka_api_secret')}\";",
        )
        .option("kafka.ssl.endpoint.identification.algorithm", "https")
        .option("kafka.client.id", "acme-supermarkets-kafka-orders-ingestion")
        .load()
    )

    return df


# ============================================================================
# TRANSFORMATION VIEWS
# ============================================================================


@dp.temporary_view(comment="Deserialize Kafka binary data to strings")
def v_kafka_orders_deserialized():
    """Deserialize Kafka binary data to strings"""
    df = spark.sql("""SELECT
  -- Kafka metadata
  topic,
  partition,
  offset,
  timestamp as kafka_timestamp,
  timestampType,

  -- Deserialize key and value
  CAST(key AS STRING) as message_key,
  CAST(value AS STRING) as message_value_json

  -- Note: For Avro, you would typically use from_avro() function
  -- Example: from_avro(value, avro_schema) as order_data

FROM $source""")

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================

# Create the streaming table
dp.create_streaming_table(
    name="acme_edw_dev.edw_bronze.kafka_orders_raw",
    comment="Raw Kafka orders data with metadata",
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
        "delta.enableChangeDataFeed": "true",
    },
)


# Define append flow(s)
@dp.append_flow(
    target="acme_edw_dev.edw_bronze.kafka_orders_raw",
    name="f_kafka_orders_bronze",
    comment="Write deserialized Kafka orders to bronze table",
)
def f_kafka_orders_bronze():
    """Write deserialized Kafka orders to bronze table"""
    # Streaming flow
    df = spark.readStream.table("v_kafka_orders_deserialized")

    return df
