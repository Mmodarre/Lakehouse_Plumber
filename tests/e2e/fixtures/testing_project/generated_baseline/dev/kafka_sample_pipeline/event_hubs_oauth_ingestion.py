# Generated by LakehousePlumber
# Pipeline: kafka_sample_pipeline
# FlowGroup: event_hubs_oauth_ingestion

from pyspark import pipelines as dp
from pyspark.sql import functions as F

# Pipeline Configuration
PIPELINE_ID = "kafka_sample_pipeline"
FLOWGROUP_ID = "event_hubs_oauth_ingestion"


# ============================================================================
# SOURCE VIEWS
# ============================================================================


@dp.temporary_view()
def v_event_hubs_telemetry_raw():
    """Load telemetry from Azure Event Hubs using OAuth authentication"""
    df = (
        spark.readStream.format("kafka")
        .option("kafka.bootstrap.servers", "my-namespace.servicebus.windows.net:9093")
        .option("subscribe", "telemetry-hub")
        .option("kafka.security.protocol", "SASL_SSL")
        .option("kafka.sasl.mechanism", "OAUTHBEARER")
        .option(
            "kafka.sasl.jaas.config",
            f"kafkashaded.org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required clientId=\"{dbutils.secrets.get(scope='azure_secrets', key='client_id')}\" clientSecret=\"{dbutils.secrets.get(scope='azure_secrets', key='client_secret')}\" scope=\"https://my-namespace.servicebus.windows.net/.default\" ssl.protocol=\"SSL\";",
        )
        .option(
            "kafka.sasl.oauthbearer.token.endpoint.url",
            "https://login.microsoft.com/12345678-1234-1234-1234-123456789012/oauth2/v2.0/token",
        )
        .option(
            "kafka.sasl.login.callback.handler.class",
            "kafkashaded.org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler",
        )
        .option("startingOffsets", "earliest")
        .option("failOnDataLoss", "false")
        .option("kafka.client.id", "databricks-event-hubs-telemetry-pipeline")
        .option("kafka.session.timeout.ms", 30000)
        .load()
    )

    # Add operational metadata columns
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


# ============================================================================
# TRANSFORMATION VIEWS
# ============================================================================


@dp.temporary_view(comment="Parse Event Hubs binary data to structured format")
def v_event_hubs_telemetry_parsed():
    """Parse Event Hubs binary data to structured format"""
    df = spark.sql("""SELECT
  CAST(key AS STRING) as telemetry_key,
  CAST(value AS STRING) as telemetry_json,
  topic,
  partition,
  offset,
  timestamp as event_hubs_timestamp,
  _processing_timestamp
FROM stream(v_event_hubs_telemetry_raw)""")

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================

# Create the streaming table
dp.create_streaming_table(
    name="acme_edw_dev.edw_bronze.event_hubs_telemetry_bronze",
    comment="Streaming table: event_hubs_telemetry_bronze",
    table_properties={
        "delta.enableChangeDataFeed": "true",
        "quality": "bronze",
        "data_source": "azure_event_hubs",
    },
)


# Define append flow(s)
@dp.append_flow(
    target="acme_edw_dev.edw_bronze.event_hubs_telemetry_bronze",
    name="f_event_hubs_telemetry_bronze",
    comment="Write Event Hubs telemetry to bronze layer",
)
def f_event_hubs_telemetry_bronze():
    """Write Event Hubs telemetry to bronze layer"""
    # Streaming flow
    df = spark.readStream.table("v_event_hubs_telemetry_parsed")

    return df
