# Generated by LakehousePlumber
# Pipeline: kafka_sample_pipeline
# FlowGroup: msk_iam_ingestion

from pyspark import pipelines as dp
from pyspark.sql import functions as F

# Pipeline Configuration
PIPELINE_ID = "kafka_sample_pipeline"
FLOWGROUP_ID = "msk_iam_ingestion"


# ============================================================================
# SOURCE VIEWS
# ============================================================================


@dp.temporary_view()
def v_msk_orders_raw():
    """Load orders from AWS MSK using IAM authentication"""
    df = (
        spark.readStream.format("kafka")
        .option(
            "kafka.bootstrap.servers",
            "b-1.msk-cluster-example.abc123.kafka.us-east-1.amazonaws.com:9098,b-2.msk-cluster-example.abc123.kafka.us-east-1.amazonaws.com:9098",
        )
        .option("subscribe", "orders")
        .option("kafka.security.protocol", "SASL_SSL")
        .option("kafka.sasl.mechanism", "AWS_MSK_IAM")
        .option(
            "kafka.sasl.jaas.config",
            "shadedmskiam.software.amazon.msk.auth.iam.IAMLoginModule required;",
        )
        .option(
            "kafka.sasl.client.callback.handler.class",
            "shadedmskiam.software.amazon.msk.auth.iam.IAMClientCallbackHandler",
        )
        .option("startingOffsets", "earliest")
        .option("failOnDataLoss", "false")
        .option("kafka.client.id", "databricks-msk-orders-pipeline")
        .option("kafka.session.timeout.ms", 30000)
        .load()
    )

    # Add operational metadata columns
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


# ============================================================================
# TRANSFORMATION VIEWS
# ============================================================================


@dp.temporary_view(comment="Parse Kafka binary data to structured format")
def v_msk_orders_parsed():
    """Parse Kafka binary data to structured format"""
    df = spark.sql(
        """SELECT
  CAST(key AS STRING) as order_key,
  CAST(value AS STRING) as order_json,
  topic,
  partition,
  offset,
  timestamp as kafka_timestamp,
  _processing_timestamp
FROM stream(v_msk_orders_raw)"""
    )

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================

# Create the streaming table
dp.create_streaming_table(
    name="acme_edw_dev.edw_bronze.msk_orders_bronze",
    comment="Streaming table: msk_orders_bronze",
    table_properties={
        "delta.enableChangeDataFeed": "true",
        "quality": "bronze",
        "data_source": "aws_msk",
    },
)


# Define append flow(s)
@dp.append_flow(
    target="acme_edw_dev.edw_bronze.msk_orders_bronze",
    name="f_msk_orders_bronze",
    comment="Write MSK orders to bronze layer",
)
def f_msk_orders_bronze():
    """Write MSK orders to bronze layer"""
    # Streaming flow
    df = spark.readStream.table("v_msk_orders_parsed")

    return df
