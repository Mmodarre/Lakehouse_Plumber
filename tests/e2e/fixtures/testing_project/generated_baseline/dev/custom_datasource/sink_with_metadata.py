# Generated by LakehousePlumber
# Pipeline: custom_datasource
# FlowGroup: sink_with_metadata

from pyspark import pipelines as dp
from pyspark.sql import functions as F

# Pipeline Configuration
PIPELINE_ID = "custom_datasource"
FLOWGROUP_ID = "sink_with_metadata"


# ============================================================================
# CUSTOM DATA SOURCE IMPLEMENTATIONS
# ============================================================================
# The following code was automatically copied from: /Users/mehdi.modarressi/Documents/Coding/Lakehouse_Plumber/tests/e2e/fixtures/testing_project/py_functions/pyspark_custom_data_sources/custom_sink_wildcard.py
# Used by action: unknown

"""Custom data sink with wildcard imports to test BaseSinkWriteGenerator operational metadata."""

from pyspark.sql.functions import *  # Wildcard import - triggers expression adaptation
from pyspark.sql.datasource import DataSink, DataSourceWriter


class CustomSinkWithWildcard(DataSink):
    """Custom sink that uses wildcard imports."""

    @classmethod
    def name(cls):
        return "custom_sink_wildcard"

    def writer(self, schema, saveMode):
        return CustomSinkWriter(schema, saveMode, self.options)


class CustomSinkWriter(DataSourceWriter):
    """Writer for custom sink with wildcard."""

    def __init__(self, schema, save_mode, options):
        self.schema = schema
        self.save_mode = save_mode
        self.options = options

    def write(self, iterator):
        # Process rows using wildcard-imported functions
        for row in iterator:
            # Simulate writing data
            pass

    def commit(self, messages):
        # Commit write operation
        pass

    def abort(self, messages):
        # Abort write operation
        pass


# Register the sink
spark.dataSource.register(CustomSinkWithWildcard)


# ============================================================================
# SOURCE VIEWS
# ============================================================================


@dp.temporary_view()
def v_test_sink_data():
    """Load test data for sink testing"""
    df = spark.sql("""SELECT 1 as id, 'test' as name, 100 as value""")

    # Add operational metadata columns
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================

# Write to custom sink with wildcard imports and operational metadata

# Register custom data sink
try:
    spark.dataSource.register(CustomSinkWithWildcard)
except Exception:
    pass  # Already registered

# Create custom sink
dp.create_sink(
    name="custom_wildcard_sink",
    format="custom_sink_wildcard",
    options={"output_path": "/tmp/custom_sink_output"},
)


# Define append flow(s)
@dp.append_flow(
    target="custom_wildcard_sink",
    name="f_custom_wildcard_sink_1",
    comment="""Write to custom sink with wildcard imports and operational metadata""",
)
def f_custom_wildcard_sink_1():
    df = spark.readStream.table("v_test_sink_data")

    # Add operational metadata columns
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df
