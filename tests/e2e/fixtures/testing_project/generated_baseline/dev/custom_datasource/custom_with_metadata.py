# Generated by LakehousePlumber
# Pipeline: custom_datasource
# FlowGroup: custom_with_metadata

from pyspark import pipelines as dp
from pyspark.sql.datasource import DataSource, DataSourceReader
from pyspark.sql.functions import *  # Wildcard import - triggers expression adaptation
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Pipeline Configuration
PIPELINE_ID = "custom_datasource"
FLOWGROUP_ID = "custom_with_metadata"


# ============================================================================
# CUSTOM DATA SOURCE IMPLEMENTATIONS
# ============================================================================
# The following code was automatically copied from: /Users/mehdi.modarressi/Documents/Coding/Lakehouse_Plumber/tests/e2e/fixtures/testing_project/py_functions/pyspark_custom_data_sources/api_with_wildcard.py
# Used by action: unknown

"""Custom data source with wildcard imports to test operational metadata import consistency."""


class APIWithWildcardSource(DataSource):
    """Custom data source that uses wildcard imports."""

    @classmethod
    def name(cls):
        return "api_wildcard"

    def schema(self):
        return StructType(
            [
                StructField("id", IntegerType(), False),
                StructField("name", StringType(), True),
                StructField("value", IntegerType(), True),
            ]
        )

    def reader(self, schema: StructType):
        return APIWithWildcardReader(schema, self.options)


class APIWithWildcardReader(DataSourceReader):
    """Reader for API with wildcard source."""

    def __init__(self, schema, options):
        self.schema = schema
        self.options = options

    def read(self, partition):
        # Generate some test data using wildcard-imported functions
        yield (1, "test_item", 100)
        yield (2, "another_item", 200)


# Register the data source
spark.dataSource.register(APIWithWildcardSource)


# ============================================================================
# SOURCE VIEWS
# ============================================================================

# Try to register the custom data source
try:
    spark.dataSource.register(APIWithWildcardSource)
except Exception:
    pass  # Ignore if already registered


@dp.temporary_view()
def v_custom_api_data():
    """Load data from custom API source with wildcard imports"""
    df = (
        spark.readStream.format("api_wildcard")
        .option("endpoint", "https://api.example.com")
        .load()
    )

    # Add operational metadata columns
    df = df.withColumn("_processing_timestamp", current_timestamp())

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================

# Create the streaming table
dp.create_streaming_table(
    name="acme_edw_dev.edw_bronze.custom_api_data",
    comment="Streaming table: custom_api_data",
)


# Define append flow(s)
@dp.append_flow(
    target="acme_edw_dev.edw_bronze.custom_api_data",
    name="f_custom_api_data",
    comment="Write custom API data to streaming table",
)
def f_custom_api_data():
    """Write custom API data to streaming table"""
    # Streaming flow
    df = spark.readStream.table("v_custom_api_data")

    return df
