# Generated by LakehousePlumber
# Pipeline: gold_load
# FlowGroup: customer_lifetime_value

from pyspark import pipelines as dp
from pyspark.sql import DataFrame

# Pipeline Configuration
PIPELINE_ID = "gold_load"
FLOWGROUP_ID = "customer_lifetime_value"


# ============================================================================
# SOURCE VIEWS
# ============================================================================


@dp.view()
def v_customer_lifetime_value_sql():
    """SQL source: customer_lifetime_value_sql"""
    df = spark.sql(
        """SELECT
  c.customer_id,
  c.name as customer_name,
  c.market_segment,
  n.name as nation,
  COUNT(DISTINCT o.order_id) as total_orders,
  SUM(o.total_price) as lifetime_value,
  AVG(o.total_price) as avg_order_value,
  MIN(o.order_date) as first_order_date,
  MAX(o.order_date) as last_order_date,
  DATEDIFF(MAX(o.order_date), MIN(o.order_date)) as customer_tenure_days
FROM acme_edw_dev.edw_silver.customer_dim c
JOIN acme_edw_dev.edw_silver.orders_fct o ON c.customer_id = o.customer_id
  AND o.order_date >= c.__start_at
  AND (o.order_date < c.__end_at OR c.__end_at IS NULL)
JOIN acme_edw_dev.edw_silver.nation_dim n ON c.nation_id = n.nation_id
  AND o.order_date >= n.__start_at
  AND (o.order_date < n.__end_at OR n.__end_at IS NULL)
GROUP BY c.customer_id, c.name, c.market_segment, n.name
"""
    )

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================


@dp.table(
    name="acme_edw_dev.edw_gold.customer_lifetime_value_mv",
    comment="Materialized view: customer_lifetime_value_mv",
    table_properties={},
)
def customer_lifetime_value_mv():
    """Write to acme_edw_dev.edw_gold.customer_lifetime_value_mv from multiple sources"""
    # Materialized views use batch processing
    df = spark.read.table("v_customer_lifetime_value_sql")

    return df
