name: gcp_autoloader_comprehensive
version: "1.0"
description: "Comprehensive GCP-specific Auto Loader template with all GCP configuration options"

parameters:
  - name: table_name
    required: true
    description: "Name of the table to ingest"
  - name: source_path
    required: true
    description: "Path to source files in Google Cloud Storage"
  - name: target_database
    required: true
    description: "Target database name"
  - name: target_table
    required: true
    description: "Target table name"
  - name: file_format
    required: true
    description: "File format (csv, json, parquet, avro, xml, orc, text, binaryFile)"

actions:
  - name: load_{{ table_name }}_gcp
    type: load
    source:
      type: cloudfiles
      path: "{{ source_path }}"
      format: "{{ file_format }}"
      options:
        # =============================================================================
        # COMMON AUTO LOADER OPTIONS
        # =============================================================================
        
        # File format (required)
        cloudFiles.format: "{{ file_format }}"
        
        # Whether to allow input directory file changes to overwrite existing data
        # Default: false
        cloudFiles.allowOverwrites: false
        
        # Auto Loader can trigger asynchronous backfills at a given interval
        # Example: "1 day" for daily backfills, "1 week" for weekly backfills
        # Do not use when cloudFiles.useManagedFileEvents is set to true
        # Default: None
        # cloudFiles.backfillInterval: "1 day"
        
        # Whether to automatically delete processed files from input directory
        # OFF: no files are deleted (default)
        # DELETE: files deleted 30 days after processing
        # MOVE: files moved to specified location after 30 days
        # Default: OFF
        cloudFiles.cleanSource: "OFF"
        
        # Amount of time to wait before processed files become candidates for cleanup
        # Must be greater than 7 days for DELETE mode
        # Default: 30 days
        cloudFiles.cleanSource.retentionDuration: "30 days"
        
        # Path to archive processed files when cleanSource is set to MOVE
        # Must be in same bucket/container as source
        # Default: None
        # cloudFiles.cleanSource.moveDestination: "/path/to/archive"
        
        # Whether to include existing files in stream processing or only new files
        # Only evaluated when starting stream for first time
        # Default: true
        cloudFiles.includeExistingFiles: true
        
        # Whether to infer exact column types when leveraging schema inference
        # Default: false
        cloudFiles.inferColumnTypes: false
        
        # Maximum number of files to process in each trigger
        # Default: 1000
        cloudFiles.maxFilesPerTrigger: 50
        
        # Maximum number of bytes to process in each trigger
        # Default: None (no limit)
        # cloudFiles.maxBytesPerTrigger: "1g"
        
        # Ignore files with modification timestamps older than this threshold
        # Format: interval string like "7 days", "1 hour"
        # Default: None
        # cloudFiles.ignoreFilesOlderThan: "7 days"
        
        # Schema evolution mode for handling new columns
        # none: disallow schema changes
        # addNewColumns: allow new columns to be added
        # rescue: rescue data that doesn't match schema
        # Default: addNewColumns
        cloudFiles.schemaEvolutionMode: "addNewColumns"
        
        # Location to store schema information
        # Default: None (uses checkpoint location)
        # cloudFiles.schemaLocation: "/path/to/schema"
        
        # Whether to collect unparseable data in a separate column
        # Default: None (rescue column not added)
        cloudFiles.rescueDataColumn: "_rescued_data"
        
        # Whether to use file notifications instead of directory listing
        # Default: false
        cloudFiles.useNotifications: true
        
        # Whether to use Databricks-managed file events
        # Default: false
        # cloudFiles.useManagedFileEvents: false
        
        # =============================================================================
        # GCP-SPECIFIC OPTIONS FOR NOTIFICATION SETUP
        # =============================================================================
        
        # The ID of the project that the GCS bucket is in
        # The Google Cloud Pub/Sub subscription will also be created within this project
        # Default: None
        cloudFiles.projectId: "my-gcp-project"
        
        # Name of the Google Cloud Pub/Sub subscription for notifications
        # If provided, Auto Loader consumes events from this subscription
        # instead of setting up its own GCS Notification and Google Cloud Pub/Sub services
        # Default: None
        # cloudFiles.subscription: "my-autoloader-subscription"
        
        # =============================================================================
        # GCP AUTHENTICATION OPTIONS
        # =============================================================================
        
        # OPTION 1: Databricks Service Credential (Recommended)
        # The name of your Databricks service credential
        # Available in Databricks Runtime 16.1 and above
        # Default: None
        # databricks.serviceCredential: "my-service-credential"
        
        # OPTION 2: Google Service Account Authentication
        # The client ID of the Google Service Account
        # Default: None
        # cloudFiles.client: "123456789012345678901"
        
        # The email of the Google Service Account
        # Default: None
        # cloudFiles.clientEmail: "my-service-account@my-project.iam.gserviceaccount.com"
        
        # The private key that's generated for the Google Service Account
        # Default: None
        # cloudFiles.privateKey: "PRIVATE KEY"
        
        # The ID of the private key that's generated for the Google Service Account
        # Default: None
        # cloudFiles.privateKeyId: "1234567890abcdef1234567890abcdef12345678"
        
        # =============================================================================
        # GCS-SPECIFIC OPTIONS
        # =============================================================================
        
        # GCS bucket name
        # Default: None
        # cloudFiles.bucketName: "my-gcs-bucket"
        
        # GCS request timeout in milliseconds
        # Default: 60000
        # cloudFiles.requestTimeout: 60000
        
        # GCS connection timeout in milliseconds
        # Default: 20000
        # cloudFiles.connectionTimeout: 20000
        
        # Maximum number of concurrent GCS requests
        # Default: 50
        # cloudFiles.maxConcurrentRequests: 50
        
        # GCS retry policy
        # Options: linear, exponential, none
        # Default: exponential
        # cloudFiles.retryPolicy: "exponential"
        
        # Maximum number of retry attempts for GCS operations
        # Default: 3
        # cloudFiles.maxRetryAttempts: 3
        
        # GCS read timeout in milliseconds
        # Default: 60000
        # cloudFiles.readTimeout: 60000
        
        # GCS write timeout in milliseconds
        # Default: 60000
        # cloudFiles.writeTimeout: 60000
        
        # Custom GCS endpoint
        # Default: None (uses standard GCS endpoints)
        # cloudFiles.endpoint: "https://storage.googleapis.com"
        
        # =============================================================================
        # GOOGLE CLOUD PUB/SUB OPTIONS
        # =============================================================================
        
        # Google Cloud Pub/Sub topic name for notifications
        # If not provided, Auto Loader will create one
        # Default: None
        # cloudFiles.pubsub.topicName: "my-autoloader-topic"
        
        # Google Cloud Pub/Sub subscription name for notifications
        # If not provided, Auto Loader will create one
        # Default: None
        # cloudFiles.pubsub.subscriptionName: "my-autoloader-subscription"
        
        # Google Cloud Pub/Sub message acknowledgment deadline in seconds
        # Default: 600
        # cloudFiles.pubsub.ackDeadlineSeconds: 600
        
        # Google Cloud Pub/Sub message retention duration in seconds
        # Default: 604800 (7 days)
        # cloudFiles.pubsub.messageRetentionDuration: 604800
        
        # Maximum number of messages to receive from Pub/Sub in one call
        # Default: 1000
        # cloudFiles.pubsub.maxReceiveCount: 1000
        
        # Google Cloud Pub/Sub flow control settings
        # Maximum number of outstanding messages
        # Default: 1000
        # cloudFiles.pubsub.flowControl.maxOutstandingMessages: 1000
        
        # Maximum number of outstanding bytes
        # Default: 1073741824 (1GB)
        # cloudFiles.pubsub.flowControl.maxOutstandingBytes: 1073741824
        
        # =============================================================================
        # GOOGLE CLOUD STORAGE NOTIFICATIONS OPTIONS
        # =============================================================================
        
        # GCS notification configuration ID
        # If not provided, Auto Loader will create one
        # Default: None
        # cloudFiles.gcs.notificationId: "my-autoloader-notification"
        
        # GCS notification payload format
        # Options: JSON_API_V1, NONE
        # Default: JSON_API_V1
        # cloudFiles.gcs.payloadFormat: "JSON_API_V1"
        
        # GCS notification event types to monitor
        # Options: OBJECT_FINALIZE, OBJECT_DELETE, OBJECT_METADATA_UPDATE
        # Default: OBJECT_FINALIZE
        # cloudFiles.gcs.eventTypes: "OBJECT_FINALIZE,OBJECT_DELETE"
        
        # GCS notification object name prefix filter
        # Default: None
        # cloudFiles.gcs.objectNamePrefix: "data/"
        
        # =============================================================================
        # GOOGLE CLOUD MONITORING OPTIONS
        # =============================================================================
        
        # Whether to enable Google Cloud Monitoring metrics for Auto Loader
        # Default: false
        # cloudFiles.cloudMonitoring.enabled: false
        
        # Google Cloud Monitoring namespace for metrics
        # Default: "Databricks/AutoLoader"
        # cloudFiles.cloudMonitoring.namespace: "Databricks/AutoLoader"
        
        # Google Cloud Monitoring metric dimensions
        # Default: None
        # cloudFiles.cloudMonitoring.dimensions: "Environment=prod,Application=DataPipeline"
        
        # =============================================================================
        # DIRECTORY LISTING OPTIONS
        # =============================================================================
        
        # Whether to list files recursively in subdirectories
        # Default: true
        recursiveFileLookup: true
        
        # Path glob filter for selecting files
        # Default: None
        # pathGlobFilter: "*.{{ file_format }}"
        
        # Modified before timestamp filter
        # Format: "yyyy-MM-dd HH:mm:ss"
        # Default: None
        # modifiedBefore: "2023-12-31 23:59:59"
        
        # Modified after timestamp filter
        # Format: "yyyy-MM-dd HH:mm:ss"
        # Default: None
        # modifiedAfter: "2023-01-01 00:00:00"
        
        # Schema hints for column data types
        # Format: "col1 TYPE1, col2 TYPE2, ..."
        # Example: "id INT, name STRING, created_at TIMESTAMP"
        cloudFiles.schemaHints: "id BIGINT, name STRING, created_at TIMESTAMP"
        
        # Whether to enable case-sensitive column matching
        # Default: true
        readerCaseSensitive: true
        
    target: v_{{ table_name }}_raw
    description: "Load {{ table_name }} from Google Cloud Storage with comprehensive Auto Loader options"

  - name: write_{{ table_name }}_target
    type: write
    source: v_{{ table_name }}_raw
    write_target:
      type: streaming_table
      database: "{{ target_database }}"
      table: "{{ target_table }}"
      comment: "Streaming table for {{ table_name }} loaded via Auto Loader from Google Cloud Storage"
      table_properties:
        "delta.autoOptimize.optimizeWrite": "true"
        "delta.enableChangeDataFeed": "true"
      description: "Write {{ table_name }} to streaming table with Auto Loader" 