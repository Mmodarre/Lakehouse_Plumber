# ==============================================================================
# LakehousePlumber Pipeline Configuration Template
# ==============================================================================
# 
# This template provides comprehensive configuration options for Delta Live Tables
# (DLT) pipelines when generating Databricks Asset Bundle resource files.
#
# Multi-Document YAML Structure:
#   - First document: project_defaults (applies to ALL pipelines)
#   - Subsequent documents: pipeline-specific configs (override defaults)
#   - Separate documents with '---' (YAML document separator)
#
# Merge Behavior:
#   - Config priority: DEFAULT → project_defaults → pipeline_specific
#   - Nested dicts are deep merged
#   - Lists are REPLACED (not appended)
#
# Usage:
#   1. Copy or rename this file to remove the .tmpl extension
#   2. Customize the settings below for your project
#   3. LHP auto-loads: templates/bundle/pipeline_config.yaml (if exists)
#   4. Or pass explicitly: lhp generate --pipeline-config config/pipeline_config.yaml
#
# Documentation: See docs/databricks_bundles.rst for detailed information
# ==============================================================================

# ==============================================================================
# PROJECT DEFAULTS - Applied to ALL pipelines unless overridden
# ==============================================================================
project_defaults:
  # -----------------------------------------------------------------------------
  # Compute Configuration
  # -----------------------------------------------------------------------------
  
  # Use serverless compute (recommended for most workloads)
  # When true: Databricks manages compute automatically
  # When false: Must specify clusters configuration
  # Default: true
  serverless: true
  
  # DLT Edition - Controls available features and pricing
  # Options: CORE, PRO, ADVANCED
  # Validation: Only these three values are allowed
  # Note: edition parameter only valid when serverless: false
  # Default: ADVANCED
  edition: ADVANCED
  
  # Runtime Channel - Controls DLT runtime version
  # Options: CURRENT (stable), PREVIEW (latest features)
  # Validation: Only these two values are allowed
  # Default: CURRENT
  channel: CURRENT
  
  # -----------------------------------------------------------------------------
  # Processing Mode
  # -----------------------------------------------------------------------------
  
  # Continuous processing (streaming mode)
  # When true: Pipeline runs continuously, processing new data as it arrives
  # When false: Pipeline runs in triggered/batch mode
  # Default: false
  continuous: false
  
  # -----------------------------------------------------------------------------
  # Notifications
  # -----------------------------------------------------------------------------
  
  # Email notifications for pipeline events
  # Applies to all pipelines unless overridden
  # Uncomment and customize as needed
  # notifications:
  #   - email_recipients:
  #       - data-engineering@company.com
  #     alerts:
  #       - on-update-failure
  #       - on-update-fatal-failure
  #       - on-update-success
  #       - on-flow-failure

---
# ==============================================================================
# EXAMPLE: Raw Ingestion Pipeline - Non-serverless with dedicated clusters
# ==============================================================================
# Uncomment and customize for pipelines requiring dedicated compute
# pipeline: raw_customer_ingestions
# serverless: false
# edition: PRO
# continuous: true
# 
# # Cluster configuration (required when serverless: false)
# clusters:
#   - label: default
#     node_type_id: Standard_D16ds_v5
#     driver_node_type_id: Standard_D32ds_v5
#     autoscale:
#       min_workers: 2
#       max_workers: 10
#       mode: ENHANCED
#     # Optional: Cluster policy ID for governance
#     policy_id: YOUR_POLICY_ID_HERE
# 
# # Event logging configuration
# event_log:
#   name: raw_ingestion_event_log
#   schema: _meta
#   catalog: ${var.default_pipeline_catalog}

---
# ==============================================================================
# EXAMPLE: Bronze Layer Pipeline - Continuous streaming with serverless
# ==============================================================================
# pipeline: bronze_load
# serverless: true
# continuous: true
# edition: ADVANCED
# 
# # Override project default notifications
# notifications:
#   - email_recipients:
#       - bronze-team@company.com
#     alerts:
#       - on-update-failure
#       - on-flow-failure
# 
# # Custom tags for cost tracking and organization
# tags:
#   layer: bronze
#   criticality: high
#   sla: 30min

---
# ==============================================================================
# EXAMPLE: Silver Layer Pipeline - Batch processing with advanced features
# ==============================================================================
# pipeline: silver_transformations
# serverless: true
# continuous: false
# edition: ADVANCED
# channel: CURRENT
# 
# # Photon engine (only for non-serverless)
# # photon: true
# 
# # Pipeline-specific notifications
# notifications:
#   - email_recipients:
#       - silver-team@company.com
#       - data-quality@company.com
#     alerts:
#       - on-update-failure
#       - on-flow-failure
# 
# # Custom tags
# tags:
#   layer: silver
#   criticality: medium
#   contains_pii: true
# 
# # Event log with custom configuration
# event_log:
#   name: silver_event_log
#   schema: _meta
#   catalog: ${var.default_pipeline_catalog}

---
# ==============================================================================
# EXAMPLE: Gold Layer Pipeline - Production-ready with strict monitoring
# ==============================================================================
# pipeline: gold_aggregations
# serverless: true
# continuous: false
# edition: ADVANCED
# 
# # Comprehensive notifications for production pipeline
# notifications:
#   - email_recipients:
#       - gold-team@company.com
#       - data-ops@company.com
#       - business-analytics@company.com
#     alerts:
#       - on-update-failure
#       - on-update-fatal-failure
#       - on-update-success
#       - on-flow-failure
# 
# # Production tags
# tags:
#   layer: gold
#   criticality: critical
#   sla: 15min
#   business_critical: true
# 
# # Event log for audit and monitoring
# event_log:
#   name: gold_event_log
#   schema: _meta
#   catalog: ${var.default_pipeline_catalog}

# ==============================================================================
# Configuration Notes
# ==============================================================================
#
# Key Constraints:
#   - edition parameter only works when serverless: false
#   - photon parameter only works when serverless: false
#   - clusters configuration required when serverless: false
#   - tags only work for non-serverless pipelines (serverless uses compute policy)
#
# Validation Rules:
#   - edition: Must be CORE, PRO, or ADVANCED
#   - channel: Must be CURRENT or PREVIEW
#   - Lists (notifications, clusters, tags) are REPLACED during merge, not appended
#
# Best Practices:
#   - Use serverless for most workloads (cost-effective, auto-scaling)
#   - Use dedicated clusters for large-scale continuous ingestion
#   - Set appropriate notifications based on pipeline criticality
#   - Use tags for cost tracking and organization (non-serverless only)
#   - Test configurations in dev before deploying to production
#
# For more information:
#   - See docs/databricks_bundles.rst
#   - Refer to Databricks DLT documentation
#
# ==============================================================================

