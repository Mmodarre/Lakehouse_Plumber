{% if mode == "append_flow" %}
@dlt.table(
    name="{{ full_table_name }}",
    comment="{{ comment }}",
    table_properties={{ properties | tojson }}
    {%- if partitions %},
    partition_cols={{ partitions | tojson }}
    {%- endif %}
    {%- if cluster_by %},
    cluster_by={{ cluster_by | tojson }}
    {%- endif %}
    {%- if table_path %},
    path="{{ table_path }}"
    {%- endif %}
)
{% if expect_all %}
@dlt.expect_all({{ expect_all | tojson }})
{% endif %}
{% if expect_all_or_drop %}
@dlt.expect_all_or_drop({{ expect_all_or_drop | tojson }})
{% endif %}
{% if expect_all_or_fail %}
@dlt.expect_all_or_fail({{ expect_all_or_fail | tojson }})
{% endif %}
def {{ table_name }}():
    """{{ description }}"""
    df = dlt.read_stream("{{ source_view }}")
    {% if add_operational_metadata %}
    
    # Add operational metadata columns
    df = df.withColumn('_ingestion_timestamp', F.current_timestamp())
    df = df.withColumn('_source_file', F.input_file_name())
    df = df.withColumn('_pipeline_run_id', F.lit(spark.conf.get("pipelines.id", "unknown")))
    df = df.withColumn('_pipeline_name', F.lit("{{ flowgroup.pipeline if flowgroup else 'unknown' }}"))
    df = df.withColumn('_flowgroup_name', F.lit("{{ flowgroup.flowgroup if flowgroup else 'unknown' }}"))
    {% endif %}
    
    return df

{% elif mode == "cdc" %}
{# 
   CDC mode: Create the streaming table first, then configure CDC flow
#}
# Create the streaming table first
dlt.create_streaming_table(
    name="{{ full_table_name }}",
    comment="{{ comment }}",
    table_properties={{ properties | tojson }}
    {%- if partitions %},
    partition_cols={{ partitions | tojson }}
    {%- endif %}
    {%- if cluster_by %},
    cluster_by={{ cluster_by | tojson }}
    {%- endif %}
    {%- if table_path %},
    path="{{ table_path }}"
    {%- endif %}
)

# CDC mode using auto_cdc
dlt.create_auto_cdc_flow(
    target="{{ full_table_name }}",
    source="{{ source_view }}",
    keys={{ cdc_config['keys'] | tojson }},
    {% if cdc_config['sequence_by'] %}sequence_by="{{ cdc_config['sequence_by'] }}",
    {% endif -%}
    stored_as_scd_type={{ cdc_config['scd_type'] | default(1) }},
    {% if cdc_config['scd_type'] == 2 and cdc_config['track_history_columns'] %}track_history_column_list={{ cdc_config['track_history_columns'] | tojson }},
    {% endif -%}
    {% if cdc_config['except_columns'] %}except_column_list={{ cdc_config['except_columns'] | tojson }},
    {% endif -%}
    {% if cdc_config['ignore_null_updates'] is not none %}ignore_null_updates={{ 'True' if cdc_config['ignore_null_updates'] else 'False' }},
    {% endif -%}
    {% if cdc_config['apply_as_deletes'] %}apply_as_deletes="{{ cdc_config['apply_as_deletes'] }}"
    {% endif -%}
)

{% else %}
{# 
   Standard streaming table: Create table first, then append flow(s)
#}
# Create the streaming table
dlt.create_streaming_table(
    name="{{ full_table_name }}",
    comment="{{ comment }}",
    table_properties={{ properties | tojson }}
    {%- if partitions %},
    partition_cols={{ partitions | tojson }}
    {%- endif %}
    {%- if cluster_by %},
    cluster_by={{ cluster_by | tojson }}
    {%- endif %}
    {%- if table_path %},
    path="{{ table_path }}"
    {%- endif %}
)

# Define append flow(s)
{% if source_views %}
{% for source_view in source_views %}
@dlt.append_flow(
    target="{{ full_table_name }}",
    name="{{ flow_name }}{% if source_views|length > 1 %}_{{ loop.index }}{% endif %}",
    {% if once %}once=True,{% endif %}
    comment="{{ description }} from {{ source_view }}"
)
{% if expect_all %}
@dlt.expect_all({{ expect_all | tojson }})
{% endif %}
{% if expect_all_or_drop %}
@dlt.expect_all_or_drop({{ expect_all_or_drop | tojson }})
{% endif %}
{% if expect_all_or_fail %}
@dlt.expect_all_or_fail({{ expect_all_or_fail | tojson }})
{% endif %}
def {{ flow_name }}{% if source_views|length > 1 %}_{{ loop.index }}{% endif %}():
    """{{ description }} from {{ source_view }}"""
    {% if once %}
    # One-time flow (backfill)
    df = spark.read.table("{{ source_view }}")
    {% else %}
    # Streaming flow
    df = spark.readStream.table("{{ source_view }}")
    {% endif %}
    {% if add_operational_metadata %}
    
    # Add operational metadata columns
    df = df.withColumn('_ingestion_timestamp', F.current_timestamp())
    df = df.withColumn('_source_file', F.input_file_name())
    df = df.withColumn('_pipeline_run_id', F.lit(spark.conf.get("pipelines.id", "unknown")))
    df = df.withColumn('_pipeline_name', F.lit("{{ flowgroup.pipeline if flowgroup else 'unknown' }}"))
    df = df.withColumn('_flowgroup_name', F.lit("{{ flowgroup.flowgroup if flowgroup else 'unknown' }}"))
    {% endif %}
    
    return df
{% endfor %}
{% else %}
# No source views provided for append flow
{% endif %}
{% endif %}