# Generated by LakehousePlumber - Bundle Resource for {{ pipeline_name }}

resources:
  pipelines:
    {{ pipeline_name }}_pipeline:
      name: {{ pipeline_name }}_pipeline
      
      # Default Catalog and Schema for the pipeline (managed via databricks.yml variables and LHP)
      catalog: ${var.default_pipeline_catalog}
      schema: ${var.default_pipeline_schema}
      
      serverless: {{ pipeline_config.serverless | lower }}
{%- if not pipeline_config.serverless and pipeline_config.get('clusters') %}

      # Compute clusters configuration
      clusters:
{%- for cluster in pipeline_config.clusters %}
        - label: {{ cluster.label }}
          node_type_id: {{ cluster.node_type_id }}
{%- if cluster.get('driver_node_type_id') %}
          driver_node_type_id: {{ cluster.driver_node_type_id }}
{%- endif %}
{%- if cluster.get('policy_id') %}
          policy_id: {{ cluster.policy_id }}
{%- endif %}
{%- if cluster.get('autoscale') %}
          autoscale:
            min_workers: {{ cluster.autoscale.min_workers }}
            max_workers: {{ cluster.autoscale.max_workers }}
{%- if cluster.autoscale.get('mode') %}
            mode: {{ cluster.autoscale.mode }}
{%- endif %}
{%- endif %}
{%- endfor %}
{%- endif %}

      libraries:
        - glob:
            include: ${workspace.file_path}/generated/${bundle.target}/{{ pipeline_name }}/**
      
      root_path: ${workspace.file_path}/generated/${bundle.target}/{{ pipeline_name }}
      
      configuration:
        bundle.sourcePath: ${workspace.file_path}/generated/${bundle.target}
{%- if pipeline_config.get('continuous') %}
      
      continuous: {{ pipeline_config.continuous | lower }}
{%- endif %}
{%- if pipeline_config.get('photon') %}
      
      photon: {{ pipeline_config.photon | lower }}
{%- endif %}
{%- if pipeline_config.get('edition') and not pipeline_config.serverless %}
      
      edition: {{ pipeline_config.edition }}
{%- endif %}
{%- if pipeline_config.get('channel') %}
      
      channel: {{ pipeline_config.channel }}
{%- endif %}
{%- if pipeline_config.get('notifications') %}
      
      notifications:
{%- for notification in pipeline_config.notifications %}
        - email_recipients:
{%- for email in notification.email_recipients %}
            - {{ email }}
{%- endfor %}
          alerts:
{%- for alert in notification.alerts %}
            - {{ alert }}
{%- endfor %}
{%- endfor %}
{%- endif %}
{%- if pipeline_config.get('tags') %}
      
      tags:
{%- for key, value in pipeline_config.tags.items() %}
        {{ key }}: {{ value }}
{%- endfor %}
{%- endif %}
{%- if pipeline_config.get('event_log') %}
      
      event_log:
        name: {{ pipeline_config.event_log.name }}
        schema: {{ pipeline_config.event_log.schema }}
        catalog: {{ pipeline_config.event_log.catalog }}
{%- endif %}

      # Additional pipeline configuration options 
      add to your pipeline_config.yaml as needed and pass the file path through the --pipeline-config flag:
 
      # Compute clusters configuration (alternative to serverless)
      # clusters:
      #   - label: default
      #     node_type_id: Standard_D16ds_v5
      #     driver_node_type_id: Standard_D32ds_v5
      #     policy_id: 1234ABCD1234ABCD
      #     autoscale:
      #       min_workers: 1
      #       max_workers: 5
      #       mode: ENHANCED
      
      # Enable continuous processing
      # continuous: false
      
   
      # Enable Photon engine only for classic computer not serverless
      # photon: true
      
      # DLT edition (CORE, PRO, ADVANCED)
      # edition: ADVANCED
      
      # Runtime channel (CURRENT, PREVIEW)
      # channel: CURRENT
      
      # Notification settings
      # notifications:
      #   - email_recipients:
      #       - user@databricks.com
      #     alerts:
      #       - on-update-success
      #       - on-update-failure
      #       - on-update-fatal-failure
      #       - on-flow-failure
      
      # Custom tags only for classic computer not serverless (serverless has its own tags through compute policy)
      # tags:
      #   tag1: val1
      
      # Event log configuration
      # event_log:
      #   name: pipeline_evenlog
      #   schema: _meta
      #   catalog: acmi_edw_dev