# Lakehouse Plumber
**Turn 4,300 lines of repetitive Python/SQL code into 50 lines of reusable YAML** üöÄ

*Because every data lake needs a good plumber to keep the flows running smoothly!* üö∞

<div align="center">
  <img src="lakehouse-plumber-logo.png" alt="LakehousePlumber Logo">
</div>

<div align="center">

[![PyPI version](https://badge.fury.io/py/lakehouse-plumber.svg?icon=si%3Apython)](https://badge.fury.io/py/lakehouse-plumber)
[![Tests](https://github.com/Mmodarre/Lakehouse_Plumber/actions/workflows/python_ci.yml/badge.svg)](https://github.com/Mmodarre/Lakehouse_Plumber/actions/workflows/python_ci.yml)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Lines of Code](https://img.shields.io/badge/lines%20of%20code-~20k-blue)](https://github.com/Mmodarre/Lakehouse_Plumber)
[![codecov](https://codecov.io/gh/Mmodarre/Lakehouse_Plumber/branch/main/graph/badge.svg?token=80IBHIFAQY)](https://codecov.io/gh/Mmodarre/Lakehouse_Plumber)
[![Documentation](https://img.shields.io/badge/docs-available-brightgreen.svg)](https://lakehouse-plumber.readthedocs.io/)
[![Databricks](https://img.shields.io/badge/Databricks-Lakeflow-%23FF3621?logo=databricks)](https://www.databricks.com/product/data-engineering)
![PyPI - Downloads](https://img.shields.io/pypi/dm/lakehouse-plumber)

</div>

**The YAML-driven Metadata framework for Databricks Lakeflow Declarative Pipelines (formerly Delta Live Tables)**

**The only Metadata framework that generates production ready Pyspark code for Lakeflow Declarative Pipelinee**

Transform repetitive data pipeline development with action-based configurations. Generate production-ready Python code from simple YAML definitions while maintaining full transparency and Databricks native features.

## ‚ö° Why Lakehouse Plumber?

### Before: Repetitive Boilerplate Hell
```python
# customer_bronze.py (86 lines √ó 50 tables = 4,300 lines!)
from pyspark.sql import functions as F
import dlt

@dlt.view()
def v_customer_raw():
    """Load customer table from raw schema"""
    df = spark.readStream.table("acmi_edw_dev.edw_raw.customer")
    df = df.withColumn('_processing_timestamp', F.current_timestamp())
    return df

@dlt.view(comment="SQL transform: customer_bronze_cleanse")
def v_customer_bronze_cleaned():
    """SQL transform: customer_bronze_cleanse"""
    return spark.sql("""SELECT
      c_custkey as customer_id,
      c_name as name,
      c_address as address,
      -- ... 50+ more lines of transformations
    FROM stream(v_customer_raw)""")

# ... 50+ more lines of data quality, table creation, etc.
```

### After: One Reusable Template
```yaml
# templates/csv_ingestion_template.yaml (50 lines for ALL tables!)
name: csv_ingestion_template
description: "Standard template for ingesting CSV files"

parameters:
  - name: table_name
    required: true
  - name: landing_folder
    required: true

actions:
  - name: load_{{ table_name }}_csv
    type: load
    source:
      type: cloudfiles
      path: "{landing_volume}/{{ landing_folder }}/*.csv"
      format: csv
      schema_evolution_mode: addNewColumns
    target: v_{{ table_name }}_raw
    
  - name: write_{{ table_name }}_bronze
    type: write
    source: v_{{ table_name }}_raw
    write_target:
      type: streaming_table
      database: "{{ catalog }}.{{ bronze_schema }}"
      table: "{{ table_name }}"
```

#### Plus 5 lines for each table
```yaml
pipeline: raw_ingestions
flowgroup: customer_ingestion

use_template: csv_ingestion_template
template_parameters:
  table_name: customer
  landing_folder: customer
  table_properties:
    PII: "true"
```


## üéØ Key Benefits

‚úÖ **Eliminate 95% of Repetitive Code** - One template replaces hundreds of nearly-identical files  
‚úÖ **Standardize Data Platform Quality** - Enforce consistent table properties, schemas, and patterns  
‚úÖ **Maintain Full Transparency** - Generated Python is readable, version-controlled, and debuggable  
‚úÖ **DataOps-Ready** - Built for CI/CD pipelines, automated testing, and multi-environment deployment  
‚úÖ **Zero Runtime Overhead** - Pure Python generation, no compilation at runtime  
‚úÖ **Databricks Native** - Works with Unity Catalog, Lakeflow UI, and Databricks Assistant  
‚úÖ **No Vendor Lock-in** - Output is standard Python & SQL you control  

## üèóÔ∏è Core Features

### üîÑ Action-Based Architecture

Build pipelines using composable **Load**, **Transform**, and **Write** actions:

#### Load Actions

- **CloudFiles**: Auto Loader for streaming files (JSON, Parquet, CSV, Avro, ORC, XML)
- **Delta**: Read from existing Delta tables with CDC support
- **JDBC**: Connect to external databases
- **SQL**: Execute custom SQL queries
- **Python**: Custom Python data sources

#### Transform Actions

- **SQL**: Standard SQL transformations
- **Python**: Custom Python transformations
- **Data Quality**: Apply expectations and validation rules
- **Schema**: Column mapping and type casting
- **Temp Table**: Create reusable temporary streaming tables

#### Write Actions

- **Streaming Table**: Live tables with change data capture
- **Materialized View**: Batch-computed analytics views

### üé® Template & Preset System

- **Templates**: Parameterized action patterns for common use cases
- **Presets**: Reusable configuration snippets for standardization
- **Environment Management**: Multi-environment support with variable substitution

### üöÄ Advanced Capabilities  

- **Append Flow API**: Efficient multi-stream ingestion with automatic table creation management
- **Smart Generation**: Content-based file writing - only regenerate what actually changed
- **Databricks Asset Bundles**: Full integration for enterprise deployment workflows
- **VS Code IntelliSense**: Complete auto-completion and validation for YAML configurations
- **Secret Management**: Secure credential handling with scope-based organization and `dbutils.secrets`
- **Operational Metadata**: Flexible metadata column creation

## üöÄ Quick Start

### Installation & Setup
```bash
# Install Lakehouse Plumber
pip install lakehouse-plumber

# Initialize new project
lhp init my_lakehouse_project --bundle
cd my_lakehouse_project

# Set up VS Code IntelliSense (optional)
lhp setup-intellisense
```

### Create Your First Pipeline
```yaml
# pipelines/bronze_ingestion/customers.yaml
  pipeline: tpch_sample_ingestion  # Grouping of generated python files in the same folder
   flowgroup: customer_ingestion   # Logical grouping for generated Python file

   actions:
      - name: customer_sample_load     # Unique action identifier
        type: load                     # Action type: Load
        readMode: stream              # Read using streaming CDF
        source:
           type: delta                # Source format: Delta Lake table
           database: "samples.tpch"   # Source database and schema in Unity Catalog
           table: customer_sample     # Source table name
        target: v_customer_sample_raw # Target view name (temporary in-memory)
        description: "Load customer sample table from Databricks samples catalog"

      - name: transform_customer_sample  # Unique action identifier
        type: transform                  # Action type: Transform
        transform_type: sql             # Transform using SQL query
        source: v_customer_sample_raw   # Input view from previous action
        target: v_customer_sample_cleaned  # Output view name
        sql: |                          # SQL transformation logic
           SELECT
           c_custkey as customer_id,    # Rename key field for clarity
           c_name as name,              # Simplify column name
           c_address as address,        # Keep address as-is
           c_nationkey as nation_id,    # Rename for consistency
           c_phone as phone,            # Simplify column name
           c_acctbal as account_balance, # More descriptive name
           c_mktsegment as market_segment, # Readable column name
           c_comment as comment         # Keep comment as-is
           FROM stream(v_customer_sample_raw)  # Stream from source view
        description: "Transform customer sample table"

      - name: write_customer_sample_bronze  # Unique action identifier
        type: write                         # Action type: Write
        source: v_customer_sample_cleaned   # Input view from previous action
        write_target:
           type: streaming_table            # Output as streaming table
           database: "{catalog}.{bronze_schema}"  # Target database.schema with substitutions
           table: "tpch_sample_customer"    # Final table name
        description: "Write customer sample table to bronze schema"
```

### Configure & Generate
```bash
# Configure environment variables
# Edit substitutions/dev.yaml with your settings

# Validate configuration
lhp validate --env dev

# Generate production-ready Python code
lhp generate --env dev --cleanup

# Deploy with Databricks Bundles (optional)
databricks bundle deploy --target dev
```

## üè¢ Enterprise Features

### Databricks Asset Bundles Integration
Full integration with Databricks Asset Bundles for enterprise-grade deployment:

```yaml
# databricks.yml - automatically detected
targets:
  dev:
    mode: development
    workspace:
      host: https://your-workspace.cloud.databricks.com
  prod:
    mode: production
    workspace:
      host: https://your-prod-workspace.cloud.databricks.com

resources:
  pipelines:
    bronze_ingestion:
      name: "bronze_ingestion_${bundle.target}"
      libraries:
        - file:
            path: ./generated/bronze_load
```

### Multi-Environment DataOps Workflow
```bash
# Development
lhp generate --env dev
databricks bundle deploy --target dev

# Staging  
lhp generate --env staging
databricks bundle deploy --target staging

# Production
lhp generate --env prod
databricks bundle deploy --target prod
```

## üìã Complete CLI Reference

### üöÄ Project Lifecycle
```bash
lhp init <project_name>           # Initialize new project with best practices
lhp validate --env <env>          # Validate all configurations before generation  
lhp generate --env <env>          # Generate production-ready Python code
lhp info                          # Show project statistics and health metrics
```

### üîç Discovery & Debugging  
```bash
lhp list-presets                  # Browse available configuration presets
lhp list-templates                # Browse available pipeline templates
lhp show <flowgroup> --env <env>  # Debug resolved configuration with substitutions
lhp stats                         # Analyze project complexity and code metrics
```

### üõ†Ô∏è State & Maintenance
```bash
lhp generate --cleanup            # Clean up orphaned generated files
lhp state --env <env>             # View generation state and dependencies  
lhp state --cleanup --env <env>   # Remove stale state and orphaned files
```

### üß† VS Code Integration
```bash
lhp setup-intellisense           # Enable full YAML auto-completion
lhp setup-intellisense --check   # Verify system prerequisites
lhp setup-intellisense --status  # Show current IntelliSense status
lhp setup-intellisense --verify  # Test IntelliSense functionality
lhp setup-intellisense --cleanup # Remove IntelliSense configuration
```

## üìÅ Project Structure

LakehousePlumber organizes your data pipeline code for maximum reusability and maintainability:

```
my_lakehouse_project/
‚îú‚îÄ‚îÄ üìã lhp.yaml                    # Project configuration
‚îú‚îÄ‚îÄ üé® presets/                    # Reusable configuration standards
‚îÇ   ‚îú‚îÄ‚îÄ bronze_layer.yaml          #   Bronze layer defaults  
‚îÇ   ‚îú‚îÄ‚îÄ silver_layer.yaml          #   Silver layer standards
‚îÇ   ‚îî‚îÄ‚îÄ gold_layer.yaml            #   Analytics layer patterns
‚îú‚îÄ‚îÄ üìù templates/                  # Parameterized pipeline patterns
‚îÇ   ‚îú‚îÄ‚îÄ standard_ingestion.yaml    #   Common ingestion template
‚îÇ   ‚îî‚îÄ‚îÄ scd_type2.yaml             #   Slowly changing dimension template
‚îú‚îÄ‚îÄ üîÑ pipelines/                  # Your pipeline definitions  
‚îÇ   ‚îú‚îÄ‚îÄ bronze_ingestion/          #   Raw data ingestion
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ customers.yaml         #     Customer data flow
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ orders.yaml            #     Order data flow
‚îÇ   ‚îú‚îÄ‚îÄ silver_transforms/         #   Business logic transformation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ customer_dimension.yaml#     Customer dimensional model
‚îÇ   ‚îî‚îÄ‚îÄ gold_analytics/            #   Analytics and reporting
‚îÇ       ‚îî‚îÄ‚îÄ customer_metrics.yaml  #     Customer analytics
‚îú‚îÄ‚îÄ üåç substitutions/              # Environment-specific values
‚îÇ   ‚îú‚îÄ‚îÄ dev.yaml                   #   Development settings
‚îÇ   ‚îú‚îÄ‚îÄ staging.yaml               #   Staging settings  
‚îÇ   ‚îî‚îÄ‚îÄ prod.yaml                  #   Production settings
‚îú‚îÄ‚îÄ ‚úÖ expectations/               # Data quality rules
‚îÇ   ‚îî‚îÄ‚îÄ customer_quality.yaml      #   Customer data validation
‚îî‚îÄ‚îÄ üêç generated/                  # Generated Python code (auto-managed)
    ‚îú‚îÄ‚îÄ bronze_load/               #   Generated bronze pipelines
    ‚îú‚îÄ‚îÄ silver_load/               #   Generated silver pipelines  
    ‚îî‚îÄ‚îÄ gold_load/                 #   Generated analytics pipelines
```

## üß† VS Code IntelliSense Support

Get powerful auto-completion, validation, and documentation for all YAML files:

### Quick Setup
```bash
lhp setup-intellisense    # One-time setup
# Restart VS Code
```

### What You Get
- ‚ö° **Smart Auto-completion** - Context-aware suggestions for all fields
- üîç **Real-time Validation** - Immediate error detection and feedback  
- üìñ **Inline Documentation** - Hover descriptions for every configuration option
- üéØ **Schema Validation** - Ensures correct YAML structure

### Supported Files
- Pipeline configurations (`pipelines/**/*.yaml`)
- Templates (`templates/**/*.yaml`) 
- Presets (`presets/**/*.yaml`)
- Environment settings (`substitutions/**/*.yaml`)
- Project configuration (`lhp.yaml`)



## üë• Who Should Use Lakehouse Plumber?

### üè¢ **Data Platform Teams**
- Standardize data engineering practices across the organization
- Enforce consistent quality, security, and operational patterns
- Reduce onboarding time for new data engineers

### üë®‚Äçüíª **Data Engineers**  
- Eliminate repetitive boilerplate code and focus on business logic
- Accelerate development with templates and presets
- Maintain code quality with automated generation and validation

### üöÄ **DevOps/Platform Engineers**
- Implement Infrastructure as Code for data pipelines
- Enable GitOps workflows with transparent Python generation
- Integrate with existing CI/CD and Databricks Asset Bundles

### üìä **Analytics Engineers**
- Build complex medallion architecture pipelines with simple YAML
- Implement advanced patterns like SCD Type 2 without coding complexity
- Focus on data modeling instead of infrastructure concerns

## üé® Advanced Patterns Made Simple

### Multi-Stream Table Creation
```yaml
# Handle multiple data sources writing to the same table
- name: write_orders_primary
  write_target:
    table: orders
    create_table: true    # Primary stream creates table
    
- name: write_orders_secondary  
  write_target:
    table: orders
    create_table: false   # Secondary streams append
```

### Data Quality Integration
```yaml
# Built-in data quality with expectations
- name: validate_customers
  type: transform
  transform_type: data_quality
  source: v_customers_raw
  expectations_file: "customer_quality.yaml"
```

<!-- ### Slowly Changing Dimensions
```yaml
# SCD Type 2 with Python transformations
- name: customer_scd2
  type: transform
  transform_type: python
  python_source: |
    def scd2_transform(df):
        return df.withColumn("__start_date", current_date()) \
                 .withColumn("__is_current", lit(True)) -->
<!-- ``` -->

## üîß Smart Multi-Stream Table Management

LakehousePlumber uses Databricks Append Flow API for efficient multi-stream ingestion:

### Simple Multi-Stream Setup
```yaml
# Multiple streams ‚Üí Single table  
- name: write_orders_primary
  write_target:
    table: orders
    create_table: true    # ‚úÖ One stream creates table

- name: write_orders_secondary  
  write_target:
    table: orders
    create_table: false   # ‚úÖ Other streams append
```

### Generated Optimized Code
```python
# Single table creation
dlt.create_streaming_table(name="orders", ...)

# Multiple append flows (high performance)
@dlt.append_flow(target="orders", name="f_orders_primary")
def f_orders_primary():
    return spark.readStream.table("v_orders_primary")

@dlt.append_flow(target="orders", name="f_orders_secondary") 
def f_orders_secondary():
    return spark.readStream.table("v_orders_secondary")
```

### Benefits
‚úÖ **Conflict Prevention** - Automatic validation ensures exactly one table creator  
‚úÖ **High Performance** - Native Databricks Append Flow API  
‚úÖ **Smart Generation** - Only regenerate files that actually changed  
‚úÖ **Clear Error Messages** - Actionable validation feedback

## üìö Real-World Examples

### ü•â Bronze: Raw Data Ingestion
```yaml
pipeline: bronze_ingestion
flowgroup: orders
presets: [bronze_layer]

actions:
  - name: load_orders_autoloader
    type: load
    source:
      type: cloudfiles
      path: "{{ landing_path }}/orders/*.parquet"
      schema_evolution_mode: addNewColumns
    target: v_orders_raw
    
  - name: write_orders_bronze
    type: write
    source: v_orders_raw
    write_target:
      type: streaming_table
      table: "orders"
      cluster_columns: ["order_date"]
```

### ü•à Silver: Business Logic & Transformations  
```yaml
pipeline: silver_transforms
flowgroup: customer_dimension

actions:
  - name: cleanse_customers
    type: transform
    transform_type: sql
    source: "{{ bronze_schema }}.customers"
    sql: |
      SELECT 
        customer_key,
        TRIM(UPPER(customer_name)) as customer_name,
        REGEXP_REPLACE(phone, '[^0-9]', '') as phone_clean,
        market_segment,
        account_balance
      FROM STREAM(LIVE.customers)
      WHERE customer_key IS NOT NULL
          
  - name: write_customer_dimension
    type: write
    source: v_customers_cleansed
    write_target:
      type: streaming_table
      table: "dim_customers"
      table_properties:
        quality: "silver"
```

### ü•á Gold: Analytics & Reporting
```yaml
pipeline: gold_analytics  
flowgroup: customer_metrics

actions:
  - name: customer_lifetime_value
    type: transform
    transform_type: sql
    source: 
      - "{{ silver_schema }}.dim_customers"
      - "{{ silver_schema }}.fact_orders"
    sql: |
      SELECT 
        c.customer_key,
        c.customer_name,
        COUNT(o.order_key) as total_orders,
        SUM(o.total_price) as lifetime_value
      FROM LIVE.dim_customers c
      LEFT JOIN LIVE.fact_orders o USING (customer_key)
      WHERE c.__is_current = true
      GROUP BY c.customer_key, c.customer_name
      
  - name: write_customer_metrics
    type: write
    source: v_customer_ltv
    write_target:
      type: materialized_view
      table: "customer_metrics"
      refresh_schedule: "0 2 * * *"  # Daily at 2 AM
```

## üöÄ Get Started Today

Ready to eliminate repetitive data pipeline code? Choose your path:

### üÜï New to Lakehouse Plumber
```bash
pip install lakehouse-plumber
lhp init my-first-project --bundle
cd my-first-project
lhp generate --env dev --cleanup
```

### üè¢ Enterprise Evaluation
- [üìñ Read the complete documentation](https://lakehouse-plumber.readthedocs.io/)
- [üî¨ Explore the ACME demo project](https://github.com/Mmodarre/acme_edw) 
- [üí¨ Join our community discussions](https://github.com/Mmodarre/Lakehouse_Plumber/discussions)

### üë• Production Deployment  
- [üèóÔ∏è Databricks Asset Bundles integration](https://lakehouse-plumber.readthedocs.io/en/latest/databricks_bundles.html)
- [üîß CI/CD best practices guide](https://lakehouse-plumber.readthedocs.io/en/latest/advanced.html)

## ü§ù Community & Support

### üí¨ Get Help
- [üìö **Documentation**](https://lakehouse-plumber.readthedocs.io/) - Complete guides and API reference
- [üêõ **Issues**](https://github.com/Mmodarre/Lakehouse_Plumber/issues) - Bug reports and feature requests  
- [üí≠ **Discussions**](https://github.com/Mmodarre/Lakehouse_Plumber/discussions) - Community Q&A and best practices

### üîß Contributing  
We welcome contributions from the community! See our [development guide](https://lakehouse-plumber.readthedocs.io/en/latest/advanced.html#development) for:
- üõ†Ô∏è Setting up local development environment
- ‚úÖ Running the comprehensive test suite  
- üìù Contributing documentation and examples
- üöÄ Submitting features and improvements

## üìÑ License & Acknowledgments

**Apache 2.0 License** - See [LICENSE](LICENSE) for details

Built with ‚ù§Ô∏è for the Databricks ecosystem and modern data engineering practices. Special thanks to the Databricks team for Lakeflow Declarative Pipelines and the open-source community for continuous inspiration.

---

<div align="center">

**Transform your data pipelines today ‚Äì [Get Started](https://lakehouse-plumber.readthedocs.io/en/latest/getting_started.html) üöÄ**

</div>
