# Generated by LakehousePlumber
# Pipeline: acmi_edw_bronze
# FlowGroup: lineitem_bronze_backfill

from pyspark.sql import functions as F
import dlt

# Pipeline Configuration
PIPELINE_ID = "acmi_edw_bronze"
FLOWGROUP_ID = "lineitem_bronze_backfill"


# ============================================================================
# SOURCE VIEWS
# ============================================================================


@dlt.view()
def v_lineitem_migration():
    """Load lineitem table from migration schema"""
    df = spark.read.table("acme_edw_prod.edw_old.lineitem")

    # Add operational metadata columns
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


# ============================================================================
# TRANSFORMATION VIEWS
# ============================================================================


@dlt.view(comment="SQL transform: lineitem_migration_cleanse")
def v_lineitem_migration_cleaned():
    """SQL transform: lineitem_migration_cleanse"""
    df = spark.sql(
        """SELECT
  l_orderkey as order_id,
  l_partkey as part_id,
  l_suppkey as supplier_id,
  l_linenumber as line_number,
  l_quantity as quantity,
  l_extendedprice as extended_price,
  l_discount as discount,
  l_tax as tax,
  l_returnflag as return_flag,
  l_linestatus as line_status,
  l_shipdate as ship_date,
  l_commitdate as commit_date,
  l_receiptdate as receipt_date,
  l_shipinstruct as ship_instruct,
  l_shipmode as ship_mode,
  l_comment as comment,
  cast(last_modified_dt as TIMESTAMP) as last_modified_dt,
  'MIGRATION' as _source_file_path,
  * EXCEPT(l_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus, l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, l_shipmode, l_comment,last_modified_dt)
FROM v_lineitem_migration"""
    )

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================


# Define append flow(s)
@dlt.append_flow(
    target="acme_edw_prod.edw_bronze.lineitem",
    name="f_lineitem_migration",
    once=True,
    comment="Append flow to acme_edw_prod.edw_bronze.lineitem",
)
def f_lineitem_migration():
    """Append flow to acme_edw_prod.edw_bronze.lineitem"""
    # One-time flow (backfill)
    df = spark.read.table("v_lineitem_migration_cleaned")

    return df
