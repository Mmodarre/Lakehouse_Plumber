# Generated by LakehousePlumber
# Pipeline: acmi_edw_bronze
# FlowGroup: orders_bronze

from pyspark.sql import functions as F
import dlt

# Pipeline Configuration
PIPELINE_ID = "acmi_edw_bronze"
FLOWGROUP_ID = "orders_bronze"



# ============================================================================
# SOURCE VIEWS
# ============================================================================

@dlt.view()
def v_orders_africa_raw():
    """Load orders Africa table from raw schema"""
    df = spark.readStream \
        .table("acme_edw_dev.edw_raw.orders_africa_raw")

    # Add operational metadata columns
    df = df.withColumn('_processing_timestamp', F.current_timestamp())

    return df

@dlt.view()
def v_orders_america_raw():
    """Load orders America table from raw schema"""
    df = spark.readStream \
        .table("acme_edw_dev.edw_raw.orders_america_raw")

    # Add operational metadata columns
    df = df.withColumn('_processing_timestamp', F.current_timestamp())

    return df

@dlt.view()
def v_orders_asia_raw():
    """Load orders Asia table from raw schema"""
    df = spark.readStream \
        .table("acme_edw_dev.edw_raw.orders_asia_raw")

    # Add operational metadata columns
    df = df.withColumn('_processing_timestamp', F.current_timestamp())

    return df

@dlt.view()
def v_orders_europe_raw():
    """Load orders Europe table from raw schema"""
    df = spark.readStream \
        .table("acme_edw_dev.edw_raw.orders_europe_raw")

    # Add operational metadata columns
    df = df.withColumn('_processing_timestamp', F.current_timestamp())

    return df

@dlt.view()
def v_orders_middle_east_raw():
    """Load orders Middle East table from raw schema"""
    df = spark.readStream \
        .table("acme_edw_dev.edw_raw.orders_middle_east_raw")

    # Add operational metadata columns
    df = df.withColumn('_processing_timestamp', F.current_timestamp())

    return df

@dlt.view()
def v_orders_migration():
    """Load orders table from migration schema"""
    df = spark.read.table("acme_edw_dev.edw_old.orders")

    # Add operational metadata columns
    df = df.withColumn('_processing_timestamp', F.current_timestamp())

    return df


# ============================================================================
# TRANSFORMATION VIEWS
# ============================================================================

@dlt.view(comment="SQL transform: orders_africa_bronze_cleanse")
def v_orders_africa_bronze_cleaned():
    """SQL transform: orders_africa_bronze_cleanse"""
    df = spark.sql("""SELECT
  o_orderkey as order_id,
  o_custkey as customer_id,
  o_orderstatus as order_status,
  o_totalprice as total_price,
  o_orderdate as order_date,
  o_orderpriority as order_priority,
  o_clerk as clerk,
  o_shippriority as ship_priority,
  o_comment as comment,
  cast(last_modified_dt as TIMESTAMP) as last_modified_dt,
  * EXCEPT(o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, last_modified_dt,_rescued_data)
FROM stream(v_orders_africa_raw)""")

    return df

@dlt.view(comment="SQL transform: orders_america_bronze_cleanse")
def v_orders_america_bronze_cleaned():
    """SQL transform: orders_america_bronze_cleanse"""
    df = spark.sql("""SELECT
  o_orderkey as order_id,
  o_custkey as customer_id,
  o_orderstatus as order_status,
  o_totalprice as total_price,
  o_orderdate as order_date,
  o_orderpriority as order_priority,
  o_clerk as clerk,
  o_shippriority as ship_priority,
  o_comment as comment,
  cast(last_modified_dt as TIMESTAMP) as last_modified_dt,
  * EXCEPT(o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, last_modified_dt,_rescued_data)
FROM stream(v_orders_america_raw)""")

    return df

@dlt.view(comment="SQL transform: orders_asia_bronze_cleanse")
def v_orders_asia_bronze_cleaned():
    """SQL transform: orders_asia_bronze_cleanse"""
    df = spark.sql("""SELECT
  o_orderkey as order_id,
  o_custkey as customer_id,
  o_orderstatus as order_status,
  o_totalprice as total_price,
  o_orderdate as order_date,
  o_orderpriority as order_priority,
  o_clerk as clerk,
  o_shippriority as ship_priority,
  o_comment as comment,
  cast(last_modified_dt as TIMESTAMP) as last_modified_dt,
  * EXCEPT(o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, last_modified_dt,_rescued_data)
FROM stream(v_orders_asia_raw)""")

    return df

@dlt.view(comment="SQL transform: orders_europe_bronze_cleanse")
def v_orders_europe_bronze_cleaned():
    """SQL transform: orders_europe_bronze_cleanse"""
    df = spark.sql("""SELECT
  o_orderkey as order_id,
  o_custkey as customer_id,
  o_orderstatus as order_status,
  o_totalprice as total_price,
  o_orderdate as order_date,
  o_orderpriority as order_priority,
  o_clerk as clerk,
  o_shippriority as ship_priority,
  o_comment as comment,
  cast(last_modified_dt as TIMESTAMP) as last_modified_dt,
  * EXCEPT(o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, last_modified_dt,_rescued_data)
FROM stream(v_orders_europe_raw)""")

    return df

@dlt.view(comment="SQL transform: orders_middle_east_bronze_cleanse")
def v_orders_middle_east_bronze_cleaned():
    """SQL transform: orders_middle_east_bronze_cleanse"""
    df = spark.sql("""SELECT
  o_orderkey as order_id,
  o_custkey as customer_id,
  o_orderstatus as order_status,
  o_totalprice as total_price,
  o_orderdate as order_date,
  o_orderpriority as order_priority,
  o_clerk as clerk,
  o_shippriority as ship_priority,
  o_comment as comment,
  cast(last_modified_dt as TIMESTAMP) as last_modified_dt,
  * EXCEPT(o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, last_modified_dt,_rescued_data)
FROM stream(v_orders_middle_east_raw)""")

    return df

@dlt.view(comment="SQL transform: orders_migration_cleanse")
def v_orders_migration_cleaned():
    """SQL transform: orders_migration_cleanse"""
    df = spark.sql("""SELECT
  o_orderkey as order_id,
  o_custkey as customer_id,
  o_orderstatus as order_status,
  o_totalprice as total_price,
  o_orderdate as order_date,
  o_orderpriority as order_priority,
  o_clerk as clerk,
  o_shippriority as ship_priority,
  o_comment as comment,
  cast(last_modified_dt as TIMESTAMP) as last_modified_dt,
  'MIGRATION' as _source_file_path,
  * EXCEPT(o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, last_modified_dt)
FROM v_orders_migration""")

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================

# Create the streaming table
dlt.create_streaming_table(
    name="acme_edw_dev.edw_bronze.orders",
    comment="Streaming table: orders")


# Define append flow(s)
@dlt.append_flow(
    target="acme_edw_dev.edw_bronze.orders",
    name="f_orders_africa_bronze",
    comment="Append flow to acme_edw_dev.edw_bronze.orders"
)
def f_orders_africa_bronze():
    """Append flow to acme_edw_dev.edw_bronze.orders"""
    # Streaming flow
    df = spark.readStream.table("v_orders_africa_bronze_cleaned")

    return df
@dlt.append_flow(
    target="acme_edw_dev.edw_bronze.orders",
    name="f_orders_america_bronze",
    comment="Append flow to acme_edw_dev.edw_bronze.orders"
)
def f_orders_america_bronze():
    """Append flow to acme_edw_dev.edw_bronze.orders"""
    # Streaming flow
    df = spark.readStream.table("v_orders_america_bronze_cleaned")

    return df
@dlt.append_flow(
    target="acme_edw_dev.edw_bronze.orders",
    name="f_orders_asia_bronze",
    comment="Append flow to acme_edw_dev.edw_bronze.orders"
)
def f_orders_asia_bronze():
    """Append flow to acme_edw_dev.edw_bronze.orders"""
    # Streaming flow
    df = spark.readStream.table("v_orders_asia_bronze_cleaned")

    return df
@dlt.append_flow(
    target="acme_edw_dev.edw_bronze.orders",
    name="f_orders_europe_bronze",
    comment="Append flow to acme_edw_dev.edw_bronze.orders"
)
def f_orders_europe_bronze():
    """Append flow to acme_edw_dev.edw_bronze.orders"""
    # Streaming flow
    df = spark.readStream.table("v_orders_europe_bronze_cleaned")

    return df
@dlt.append_flow(
    target="acme_edw_dev.edw_bronze.orders",
    name="f_orders_middle_east_bronze",
    comment="Append flow to acme_edw_dev.edw_bronze.orders"
)
def f_orders_middle_east_bronze():
    """Append flow to acme_edw_dev.edw_bronze.orders"""
    # Streaming flow
    df = spark.readStream.table("v_orders_middle_east_bronze_cleaned")

    return df
@dlt.append_flow(
    target="acme_edw_dev.edw_bronze.orders",
    name="f_orders_migration",
once=True,    comment="Append flow to acme_edw_dev.edw_bronze.orders"
)
def f_orders_migration():
    """Append flow to acme_edw_dev.edw_bronze.orders"""
    # One-time flow (backfill)
    df = spark.read.table("v_orders_migration_cleaned")

    return df
