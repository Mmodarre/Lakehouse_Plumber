.. Lakehouse Plumber documentation master file

====================================
Lakehouse Plumber
====================================

Lakehouse Plumber turns concise YAML **actions** into fully-featured
Databricks Lakeflow Declarative Pipelines(Formerly Delta Live Tables) – without hiding the Databricks
platform you already know and love.

.. contents:: Page Outline
   :depth: 2
   :local:



Why Lakehouse Plumber?
======================

**Core principles of a good data engineering framework:**

✅ **It should…**

- **Eliminate repetitive boiler-plate** so you spend time on business logic, not plumbing.

- **Standardise Lakehouse platform and quality** – For instance, all tables in Bronze layer should have the same Delta table properties.

- **Remain transparent** – the generated Python files are readable, version-controlled and exactly what runs in production.

- **Fit naturally into DataOps workflows** (CI/CD, automated testing, environments).

- **Be easy to debug** – The generated Python code is readable and can be debugged in the Databricks IDE for faster troubleshooting and development cycles.

- **Provide a pathway to data democratisation** – Allow power users and non-technical teams to create artifacts while upholding platform standards.

❌ **And it should NOT…**

- **Introduce runtime overhead** (no compiling configurations at runtime to generate pipelines).

- **Obscure or wrap Databricks features** – you still work with notebooks, Unity Catalog, DLT UI.

- **Make debugging harder** – any failures can be debugged using Databricks' IDE and AI (Databricks Assistant).

- **Lock you in** – the output is plain Python & SQL that you control.


**Real-World Example**

Instead of repeating load code like this **50 times for 50 tables**:

.. code-block:: python
   :caption: customer_bronze.py (86 lines × 50 tables = 4,300 lines!)
   :linenos:

   # Generated by LakehousePlumber
   from pyspark.sql import functions as F
   import dlt
   
   @dlt.view()
   def v_customer_raw():
       """Load customer table from raw schema"""
       df = spark.readStream \
           .table("acmi_edw_dev.edw_raw.customer")
       df = df.withColumn('_processing_timestamp', F.current_timestamp())
       return df
   
   @dlt.view(comment="SQL transform: customer_bronze_cleanse")
   def v_customer_bronze_cleaned():
       """SQL transform: customer_bronze_cleanse"""
       return spark.sql("""SELECT
         c_custkey as customer_id,
         c_name as name,
         c_address as address,
         -- ... 50+ more lines of transformations
       FROM stream(v_customer_raw)""")
   
   # ... 50+ more lines of data quality, table creation, etc.

We create **one reusable template**:

.. code-block:: yaml
   :caption: templates/csv_ingestion_template.yaml (50 lines, used for ALL tables)
   :linenos:

   name: csv_ingestion_template
   description: "Standard template for ingesting CSV files"
   
   parameters:
     - name: table_name
       required: true
     - name: landing_folder
       required: true
   
   actions:
     - name: load_{{ table_name }}_csv
       type: load
       source:
         type: cloudfiles
         path: "{landing_volume}/{{ landing_folder }}/*.csv"
         format: csv
         options:
           header: True
           delimiter: "|"
           cloudFiles.schemaEvolutionMode: "addNewColumns"
       target: v_{{ table_name }}_cloudfiles
   
     - name: write_{{ table_name }}_cloudfiles
       type: write
       source: v_{{ table_name }}_cloudfiles
       write_target:
         type: streaming_table
         database: "{catalog}.{raw_schema}"
         table: "{{ table_name }}"

And our pipeline code becomes this simple **5-line configuration per table**:

.. code-block:: yaml
   :caption: customer_ingestion.yaml (5 lines per table)
   :emphasize-lines: 4,5

   pipeline: raw_ingestions
   flowgroup: customer_ingestion
   
   use_template: csv_ingestion_template
   template_parameters:
     table_name: customer
     landing_folder: customer

**Result:** 4,300 lines of repetitive Python → 250 lines total (1 template + 50 simple configs)


Core Workflow
=============

The execution model is deliberately simple:

.. mermaid::

   graph LR
       A[Load] --> B{0..N Transform}
       B --> C[Write]

1. **Load**    Ingest raw data from CloudFiles, Delta, JDBC, SQL, or custom Python.
2. **Transform**   Apply *zero or many* transforms (SQL, Python, schema, data-quality, temp-tables…).
3. **Write**   Persist results as Streaming Tables, Materialized Views, or Snapshots.

Features at a Glance
====================

* **Actions** – Load | Transform | Write with many sub-types (see :doc:`actions_reference`).
* **Presets & Templates** – reuse patterns without copy-paste.
* **Substitutions** – environment-aware tokens & secret references.
* **CDC & SCD** – change-data capture SCD type 1 and 2 and snapshot ingestion.
* **Append Flows** – append data to existing streaming tables.(multi source write to single target table)
* **Data-Quality** – declarative expectations integrated into transforms.
* **Operational Metadata** – custom audit columns and metadata.
* **Smart State Management** – regenerate only what changed; cleanup orphaned code. (Terraform-like state management)
* **IntelliSense** – VS Code schema hints & YAML completion.
* **Seeding** – seed data from existing tables using Lakeflow native features. 

What is the output of Lakehouse Plumber?
========================================

Lakehouse Plumber generates Python files that can be used to create Databricks Lakeflow Declarative Pipelines.


.. code-block:: python
   :caption: customer_ingestion.py (Generated by Lakehouse Plumber)
   :linenos:

   # Generated by LakehousePlumber
   # Pipeline: raw_ingestions
   # FlowGroup: customer_ingestion

   from pyspark.sql import functions as F
   import dlt

   # Pipeline Configuration
   PIPELINE_ID = "raw_ingestions"
   FLOWGROUP_ID = "customer_ingestion"

   # ============================================================================
   # SOURCE VIEWS
   # ============================================================================

   # Schema hints for customer_cloudfiles table
   customer_cloudfiles_schema_hints = """
      c_custkey BIGINT,
      c_name STRING,
      c_address STRING,
      c_nationkey BIGINT,
      c_phone STRING,
      c_acctbal DECIMAL(18,2),
      c_mktsegment STRING,
      c_comment STRING
      """.strip().replace("\n", " ")


   @dlt.view()
   def v_customer_cloudfiles():
    """Load customer CSV files from landing volume"""
    df = spark.readStream \
        .format("cloudFiles") \
        .option("cloudFiles.format", "csv") \
        .option("header", True) \
        .option("delimiter", "|") \
        .option("cloudFiles.maxFilesPerTrigger", 11) \
        .option("cloudFiles.inferColumnTypes", False) \
        .option("cloudFiles.schemaEvolutionMode", "addNewColumns") \
        .option("cloudFiles.rescuedDataColumn", "_rescued_data") \
        .option("cloudFiles.schemaHints", customer_cloudfiles_schema_hints) \
        .load("/Volumes/acmi_edw_dev/edw_raw/landing_volume/customer/*.csv")


      # Add operational metadata columns
      df = df.withColumn('_record_hash', F.xxhash64(*[F.col(c) for c in df.columns]))
      df = df.withColumn('_source_file_size', F.col('_metadata.file_size'))
      df = df.withColumn('_source_file_modification_time', F.col('_metadata.file_modification_time'))
      df = df.withColumn('_source_file_path', F.col('_metadata.file_path'))

    return df


   # ============================================================================
   # TARGET TABLES
   # ============================================================================

   # Create the streaming table
   dlt.create_streaming_table(
      name="acmi_edw_dev.edw_raw.customer",
      comment="Streaming table: customer",
      table_properties={"delta.autoOptimize.optimizeWrite": "true", "delta.enableChangeDataFeed": "true"})


   # Define append flow(s)
   @dlt.append_flow(
      target="acmi_edw_dev.edw_raw.customer",
      name="f_customer_cloudfiles",
      comment="Append flow to acmi_edw_dev.edw_raw.customer"
   )
   def f_customer_cloudfiles():
      """Append flow to acmi_edw_dev.edw_raw.customer"""
      # Streaming flow
      df = spark.readStream.table("v_customer_cloudfiles")

      return df



Next Steps
==========

* :doc:`getting_started` – a hands-on walk-through using the ACMI demo project.
* :doc:`concepts` – deep-dive into FlowGroups, Actions, presets, templates and more.
* :doc:`cli` – command-line reference.

.. toctree::
   :maxdepth: 2
   :hidden:

   getting_started
   concepts
   actions_reference
   advanced
   cli
   examples
   api 